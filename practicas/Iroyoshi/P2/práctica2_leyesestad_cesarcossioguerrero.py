# -*- coding: utf-8 -*-
"""Práctica2_LeyesEstad_CesarCossioGuerrero.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZaCESuB8zvSg0CDRxYwLgWLv-yvIJRUg

## Práctica 2: Propiedades estadísticas de la lengua

### Cesar Cossio Guerrero

### Fecha de entrega: 2 de Marzo de 2025 11:59pm

1. Verificar si la ley de Zipf se cumple en un lenguaje artificial creado por ustedes.
    - *Ejemplo:* Un "lenguaje artificial" podría ser simplemente un texto donde las secuencias de caracteres fueron generadas aleatoriamente.

### Carga de paqueterías y funciones necesarias

#Ejercicio 1. Verificar si la ley de Zipf se cumple en un lenguaje artificial creado por ustedes.

### La aproximación que hago a este problema es proponer generar un lenguaje artificial de máquina, es decir, con $0$'s y $1$'s.
"""

pip install jupytext

"""### Por alguna razón si existe un registro en el caché de los datasets de huggingface no los carga de manera adecuada. Por ello pido que se corra el siguiente código y se reinicié la sesión un única vez. Gracias. La celda a la que me refiero es del segundo ejercicio y es para cargar los documentos
*Data_Sonetos = load_dataset("biglam/spanish_golden_age_sonnets",trust_remote_code=True)*
"""

!pip install datasets

pip install -U datasets

### Carga de Libererías
import random
from collections import Counter
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [10, 6]
import numpy as np
import pandas as pd

"""### Ejemplo del lenguaje artificial que voy a trabajar"""

Vacabulario = ["0", "1", " "]
# Las probabilidades de escoger cada caracter es de 0.3.
corpus_ejemplo = random.choices(Vacabulario, weights = [3, 3, 3], k = 50)
print('Estas son el tipo de palabras que tiene este lenguaje \n')
print("".join(corpus_ejemplo).split())

"""### A continuación genero un corpus completo"""

random.seed(50)
pre_corpus = random.choices(Vacabulario, weights = [3, 3, 3], k = 500000)
corpus = "".join(pre_corpus)
Palabras = corpus.split()
print('El número de tokens es: \n')
len(Palabras)

"""### Calculemos el tamaño de palabras diferentes"""

Vocabulary = Counter(Palabras)
Vocabulary.most_common(10)
len(Vocabulary)

"""### Utilizo las funciones vistas en clase para cuantificar que tan bien se cumple la ley Zipf en este corpus."""

def get_frequencies(vocabulary: Counter, n: int) -> list:

    return [_[1] for _ in vocabulary.most_common(n)]

def plot_frequencies(frequencies: list, title="Freq of words", log_scale=False):
    x = list(range(1, len(frequencies)+1))
    plt.plot(x, frequencies, "-v")
    plt.xlabel("Freq rank (r)")
    plt.ylabel("Freq (f)")
    if log_scale:
        plt.xscale("log")
        plt.yscale("log")
    plt.title(title)

"""### Se calculan las frecuencias relativas"""

frequencies = get_frequencies(Vocabulary, 100)
#plot_frequencies(frequencies)
#plot_frequencies(frequencies, log_scale=True)

"""### Ahora se calcula los coeficientes de una regresión simple a los logaritmos de las frecuencias contra los rangos"""

from scipy.optimize import minimize

ranks = np.array([i for i in range(len(frequencies))]) + 1
frecs = np.array(frequencies)

# Inicialización
a0 = 1

# Función de minimización:
func = lambda a: sum((np.log(frecs)-(np.log(frecs[0])-a*np.log(ranks)))**2)

# Apliando minimos cuadrados
a_hat = minimize(func, a0).x[0]

print('alpha:', a_hat, '\nMSE:', func(a_hat))
def plot_generate_zipf(alpha: np.float64, ranks: np.array, freqs: np.array) -> None:
    plt.plot(np.log(ranks),  np.log(freqs[0]) - alpha*np.log(ranks), color='r', label='Aproximación Zipf')

plot_generate_zipf(a_hat, ranks, frecs)
plt.plot(np.log(ranks),np.log(frecs), color='b', label='Distribución CREA')
plt.xlabel('log ranks')
plt.ylabel('log frecs')
plt.legend(bbox_to_anchor=(1, 1))
plt.show()

"""### Podemos notar que el valor de $\alpha$ es muy parecido al del corpues CREA que vimos en la práctica. También notamos que si está presente la tendencia de una linea recta! Esto apesar de haber inventado un lenguaje de manera artificial!

### También repetí el mismo ejercicio utilizando diferentes probabilidades para los caracteres del vocabulario y el resultado siempre seguía la misma tendencia, es muy sorprendente!

# Ejercicio 2: Realizar reconocimiento de entidades nombradas (NER) de HuggingFace

2. Explorar `datasets` del sitio [Hugging Face](https://huggingface.co/datasets) y elegir documentos de diferentes dominios en Español. Realizar reconocimiento de entidades nombradas (NER).
    - Pueden utilizar subconjuntos de los datasets encontrados
    - Mostrar resultados del reconocimiento
    - Una distribución de frecuencias de las etiquetas más comunes en cada dominio
    - Comentarios generales del desempeño observado.

*Sugerencias: Spacy, CoreNLP (puede ser cualquier otra herramienta)*

### Se inicializan las funcionalidades así como librerías necesarias, también una función que permite obtener histogramas con las etiquetas más comunes en cada dataset.
"""

# Se debe reiniciar la sesión luego de correr por primera vez esta celda
# Y no hace falta correrla dos veces
!python -m spacy download es_core_news_md

from datasets import load_dataset
import spacy
import random
from collections import defaultdict
import matplotlib.pyplot as plt
from spacy import displacy

"""### Cargamos un pipeline especializado en Español"""

nlp = spacy.load("es_core_news_md")

"""### Estas son las ligas a las bases de datos de texto en Español

**Sonetos de la era dorada**
"""

#https://huggingface.co/datasets/biglam/spanish_golden_age_sonnets

"""**Tweets**"""

#https://huggingface.co/datasets/pysentimiento/spanish-tweets

"""**Recopilación de Texto en Españo con 1.5 billones de palabras**"""

#https://huggingface.co/datasets/jhonparra18/spanish_billion_words_clean

"""**Chistes**"""

#https://huggingface.co/datasets/mrm8488/CHISTES_spanish_jokes

"""### Se cargan los documentos en forma de data"""

Data_Sonetos = load_dataset("biglam/spanish_golden_age_sonnets",trust_remote_code=True)
Data_Tweets = load_dataset("alexfabbri/multi_news",trust_remote_code=True)
Data_BillionWords = load_dataset("alexfabbri/multi_news",trust_remote_code=True)
Data_Chistes = load_dataset("mrm8488/CHISTES_spanish_jokes",trust_remote_code=True)

"""### Veamos como se ve el etiquetado de algunos chistes"""

cc=list(nlp.pipe(Data_Chistes["train"]['text']))

displacy.render(cc[1:10], style="ent")

"""### Definimos una función para crear los histogramas de las categorías más comunes"""

def Hist_NER(data,column,ax,K=100):

    Dict_NER = defaultdict(int)
    corpus = random.choices(data["train"][column], k=K)
    docs = list(nlp.pipe(corpus))
    for j, doc in enumerate(docs):
        for i, ent in enumerate(doc.ents):
            Dict_NER[ent.label_] += 1

    sorted_items = sorted(Dict_NER.items(), key=lambda item: item[1], reverse=True)
    sorted_keys = [item[0] for item in sorted_items]
    sorted_values = [item[1] for item in sorted_items]

    ax.bar(sorted_keys, sorted_values)
    ax.set_xlabel("Names")
    ax.set_ylabel("Frequency")
    ax.set_title("Frequency Distribution")
    # Adjust x-tick labels if needed (e.g., rotate them if they're long)
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right")

"""### Gráfico con los histogramas de 4 campos distintos: Sonetos de la época dorada de México, Tweets, Billion words, y de Chistes."""

fig, axes = plt.subplots(2, 2, figsize=(12, 8))
# Plot the histograms on each subplot
Hist_NER(Data_Sonetos, 'sonnet_text', axes[0, 0])
Hist_NER(Data_Tweets, 'document', axes[0, 1], K=10)
Hist_NER(Data_BillionWords, 'document', axes[1, 0], K=50)
Hist_NER(Data_Chistes, 'text', axes[1, 1])

# Adjust layout to prevent overlapping titles and labels
plt.tight_layout()
plt.show()

"""### Explicación de cada categoría"""

spacy.explain("MISC")

spacy.explain("PER")

spacy.explain("ORG")

spacy.explain("LOC")

"""## Conclusiones

### Lo que más me llamó la atención son las muy pocas categorías que existen en el pipeline de Spacy en Español comparado con las que vimos en clase para el Inglés. Creo que aún hya una gran diferencia.

### La reducida cantidad de categorías se debe a que muchas categorías que están divididas en el Inglés no aparecen así en el Español. Esto hace que sea más difícil crear un etiquetado específico.
"""

