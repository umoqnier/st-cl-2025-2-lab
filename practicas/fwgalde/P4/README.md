# PrÃ¡ctica 4: Modelos del Lenguaje Neuronales ğŸ§ ğŸ’»
## Entorno de desarrollo ğŸŒ
- Fedora Linux 40 (Workstation Edition)
- Python 3.11.11 ğŸ
- Jupyter ğŸ““
  - IPython          : 8.30.0
  - ipykernel        : 6.29.5
  - jupyter_client   : 8.6.3
  - jupyter_core     : 5.7.2
  - jupyter_server   : 2.15.0
  - jupyterlab       : 4.3.5
  - nbclient         : 0.10.2
  - nbconvert        : 7.16.6
  - nbformat         : 5.10.4
  - traitlets        : 5.14.3

## Dependencias ğŸ“¦
Para el correcto funcionamiento del programa se necesitan las siguientes dependencias:

- numpy=1.26.4
- nltk=3.9.1
- pytorch=2.4.1
- time â³
- random ğŸ²
- randint ğŸ”¢
- matplotlib=3.10.1 ğŸ“Š
- scikit-learn=1.6.1 ğŸ¤–
- umap-learn=0.5.7 ğŸ§©
- gensim=4.3.3 ğŸ§ 
- tokenizers=0.21.1 ğŸ“

Para mÃ¡s informaciÃ³n se puede consultar el documento [requirements.yml](requirements.yml) ğŸ“„

## Notas ğŸ“
- Se tuvieron que descargar lo siguientes elementos de nltk:

```
nltk.download('reuters')
nltk.download('punkt_tab')
nltk.download('stopwords')
```

- Se utilizaron LLM's para la realizaciÃ³n de documentaciÃ³n y en especial en la creaciÃ³n de funciones que me permitieran adaptar el modelo de subword tokenization; es decir, el extra. ğŸ”§
