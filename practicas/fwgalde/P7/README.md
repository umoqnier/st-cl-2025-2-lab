# PrÃ¡ctica 7: ConstrucciÃ³n de un *Retrieval-augmented Generation (RAG) especializado* especializado

# StudyBuddy Documentation

## Tutorial: Â¡Primeros pasos con StudyBuddy!
1. **Clona el repositorio**:

   ```bash
   git clone https://github.com/fwgalde/st-cl-2025-2-lab.git
   cd practicas/fwgalde/P7
   ```
2. **Instala dependencias**:

   ```bash
   mamba env create --name <entorno> -f requirements.yml
   ```
3. **Descarga modelos** (asegÃºrate de tener Ollama corriendo):

   ```bash
   ollama pull gemma3:4b
   ```
4. **Inicia la aplicaciÃ³n**:

   ```bash
   streamlit run app.py
   ```
5. **Carga documentos** en la barra lateral (PDF/TXT). Haz clic en **Indexar nuevos docs**. En el caso de prueba utilizamos las notas de clase.
6. **Pregunta** usando la interfaz de chat.

Â¡Listo! Aprende haciendo tu primera consulta.

---

## How-To: Tareas comunes

### Indexar documentos nuevos

1. Sube archivos desde la barra lateral.
2. Pulsa **Indexar nuevos docs**.
3. Observa el feedback sobre archivos procesados.

### Reiniciar todo el estudio

* Pulsa **Reiniciar estudio** para borrar documentos e Ã­ndice. La base de conocimiento quedarÃ¡ vacÃ­a. DespuÃ©s vas a tener que cerrar la aplicaciÃ³n desde la terminal con `C-c` y ejecutar nuevamente el comando `streamlit run app.py`.

### Ajustar parÃ¡metros del modelo

* En la barra lateral, usa el slider **Top k** para cambiar cuÃ¡ntos fragments considera el LLM.
* Modifica variables de entorno `OLLAMA_MODEL` o en `app.py` para cambiar modelo o temperatura.

---

## Explanation: Arquitectura y conceptos

StudyBuddy implementa un sistema **RAG (Retrieval-Augmented Generation)** local:

1. **Ingesta**: carga documentos (TXT/PDF), los fragmenta en trozos de texto.
2. **Embeddings**: convierte fragments en vectores semÃ¡nticos usando `all-MiniLM-L6-v2`.
3. **Vector Store**: almacena vectores en **ChromaDB** local, persistente en disco.
4. **RecuperaciÃ³n**: ante una pregunta, recupera los *k* fragments mÃ¡s similares.
5. **GeneraciÃ³n**: envÃ­a contexto + pregunta al LLM (e.g. `gemma3:4b` en Ollama) para obtener la respuesta.


### Componentes principales

* **Streamlit**: interfaz GUI.
* **LangChain**: orquestra loaders, splitter, embeddings, vectorstore y chains.
* **Ollama**: servidor local de LLM.
* **Chroma**: base de datos vectorial ligera.

---

## Reference: ConfiguraciÃ³n y API

| ParÃ¡metro         | DescripciÃ³n                                  | Valor por defecto     |
| ----------------- | -------------------------------------------- | --------------------- |
| `DOCS_PATH`       | Carpeta para documentos subidos              | `./docs`              |
| `PERSIST_DIR`     | Carpeta de ChromaDB                          | `./db/chroma`         |
| `EMBEDDING_MODEL` | Nombre del modelo de embeddings              | `all-MiniLM-L6-v2`    |
| `LLM_MODEL`       | Identificador del modelo en Ollama           | `gemma3:4b`           |
| `LLM_TEMPERATURE` | Temperatura del muestreo del LLM             | `0.2`                 |
| `CHUNK_SIZE`      | TamaÃ±o de fragmento en caracteres            | `1500`                |
| `CHUNK_OVERLAP`   | Solapamiento entre fragments                 | `300`                 |
| `TOP_K`           | NÃºmero de fragments recuperados por consulta | `3` (slider en la UI) |

---

## ReflexiÃ³n: Limitaciones y consideraciones Ã©ticas

Aunque **StudyBuddy** ofrece una potente herramienta de estudio basada en RAG, es importante reconocer sus limitaciones y los posibles riesgos sociales:

1. **Alucinaciones y veracidad**

   * El LLM puede inventar informaciÃ³n (Â«hallucinationsÂ») al no distinguir claramente entre hechos y sintetizaciones creativas.

2. **ProtecciÃ³n de datos y privacidad**

   * Al ingestar documentos personales, existe riesgo de exponer datos sensibles si la app se comparte o se aloja en la nube.

3. **Implicaciones Ã©ticas**

   * Dependencia excesiva en IA: los estudiantes podrÃ­an sustituir el pensamiento crÃ­tico por respuestas automÃ¡ticas.
   * Desigualdad de acceso: requiere hardware relativamente potente (RAM, CPU) para correr localmente, lo cual puede excluir a usuarios con recursos limitados.

> **ConclusiÃ³n**: StudyBuddy es una herramienta valiosa para potenciar el aprendizaje, pero debe usarse con **conciencia crÃ­tica**, complementada con supervisiÃ³n humana y prÃ¡cticas responsables para minimizar riesgos de falta de veracidad, privacidad y exclusiÃ³n social.

---

## Entorno de desarrollo ğŸŒ
- Fedora Linux 42 (Worstation Edition) ğŸ§
- Python 3.11.12 ğŸ
- Jupyter ğŸ““
  - ipywidgets=8.1.7 ğŸ›ï¸
  - jupyter_widget   : 3.0.15
  - jupyter_pygment   : 0.3.0
  - jupyter_server   : 2.27.3 ğŸŒ
  - jupyterlab       : 4.4.3 ğŸ–¥ï¸

## Dependencias ğŸ“¦
Para el correcto funcionamiento del programa se necesitan las siguientes dependencias:

- chromadb=1.0.9
- langchain=0.3.25
- langchain-chroma=0.2.4
- langchain-community=0.3.24
- langchain-huggingface=0.2.0
- langchain-text-splitters=0.3.8
- loguru=0.7.3
- rich=14.0.0 ğŸ¨
- pypdf=5.5.0
- sentence-transformers=4.1.0
- streamlit=1.45.1
- langchain-ollama=0.3.3 ï¸

Para mÃ¡s informaciÃ³n se puede consultar el documento [requirements.yml](requirements.yml) ğŸ“„

---

## Integrantes
- [fwgalde](https://github.com/fwgalde)
- [EARSV](https://github.com/EARSV)

## Notas ğŸ“
- Se utilizaron LLMâ€™s ğŸ¤– para la realizaciÃ³n de documentaciÃ³n y formato del cÃ³digo. AsÃ­ como la realizaciÃ³n de esta documentaciÃ³n tipo DiÃ¡taxis.
- La aplicaciÃ³n para Hugginface fue realizada con el SDK de Gradio y se puede ver el diseÃ±o de la "interfaz" en el archivo [app.py](app.py).