# Pr√°ctica 7: Construcci√≥n de un *Retrieval-augmented Generation (RAG) especializado* especializado

# StudyBuddy Documentation

## Tutorial: ¬°Primeros pasos con StudyBuddy!
1. **Clona el repositorio**:

   ```bash
   git clone https://github.com/fwgalde/st-cl-2025-2-lab.git
   cd practicas/fwgalde/P7
   ```
2. **Instala dependencias**:

   ```bash
   mamba env create --name <entorno> -f requirements.yml
   ```
3. **Descarga modelos** (aseg√∫rate de tener Ollama corriendo):

   ```bash
   ollama pull gemma3:4b
   ```
4. **Inicia la aplicaci√≥n**:

   ```bash
   streamlit run app.py
   ```
5. **Carga documentos** en la barra lateral (PDF/TXT). Haz clic en **Indexar nuevos docs**. En el caso de prueba utilizamos las notas de clase.
6. **Pregunta** usando la interfaz de chat.

¬°Listo! Aprende haciendo tu primera consulta.

---

## How-To: Tareas comunes

### Indexar documentos nuevos

1. Sube archivos desde la barra lateral.
2. Pulsa **Indexar nuevos docs**.
3. Observa el feedback sobre archivos procesados.

### Reiniciar todo el estudio

* Pulsa **Reiniciar estudio** para borrar documentos e √≠ndice. La base de conocimiento quedar√° vac√≠a. Despu√©s vas a tener que cerrar la aplicaci√≥n desde la terminal con `C-c` y ejecutar nuevamente el comando `streamlit run app.py`.

### Ajustar par√°metros del modelo

* En la barra lateral, usa el slider **Top k** para cambiar cu√°ntos fragments considera el LLM.
* Modifica variables de entorno `OLLAMA_MODEL` o en `app.py` para cambiar modelo o temperatura.

---

## Explanation: Arquitectura y conceptos

StudyBuddy implementa un sistema **RAG (Retrieval-Augmented Generation)** local:

1. **Ingesta**: carga documentos (TXT/PDF), los fragmenta en trozos de texto.
2. **Embeddings**: convierte fragments en vectores sem√°nticos usando `all-MiniLM-L6-v2`.
3. **Vector Store**: almacena vectores en **ChromaDB** local, persistente en disco.
4. **Recuperaci√≥n**: ante una pregunta, recupera los *k* fragments m√°s similares.
5. **Generaci√≥n**: env√≠a contexto + pregunta al LLM (e.g. `gemma3:4b` en Ollama) para obtener la respuesta.


### Componentes principales

* **Streamlit**: interfaz GUI.
* **LangChain**: orquestra loaders, splitter, embeddings, vectorstore y chains.
* **Ollama**: servidor local de LLM.
* **Chroma**: base de datos vectorial ligera.

---

## Reference: Configuraci√≥n y API

| Par√°metro         | Descripci√≥n                                  | Valor por defecto     |
| ----------------- | -------------------------------------------- | --------------------- |
| `DOCS_PATH`       | Carpeta para documentos subidos              | `./docs`              |
| `PERSIST_DIR`     | Carpeta de ChromaDB                          | `./db/chroma`         |
| `EMBEDDING_MODEL` | Nombre del modelo de embeddings              | `all-MiniLM-L6-v2`    |
| `LLM_MODEL`       | Identificador del modelo en Ollama           | `gemma3:4b`           |
| `LLM_TEMPERATURE` | Temperatura del muestreo del LLM             | `0.2`                 |
| `CHUNK_SIZE`      | Tama√±o de fragmento en caracteres            | `1500`                |
| `CHUNK_OVERLAP`   | Solapamiento entre fragments                 | `300`                 |
| `TOP_K`           | N√∫mero de fragments recuperados por consulta | `3` (slider en la UI) |

---

## Entorno de desarrollo üåê
- Fedora Linux 42 (Worstation Edition) üêß
- Python 3.11.12 üêç
- Jupyter üìì
  - ipywidgets=8.1.7 üéõÔ∏è
  - jupyter_widget   : 3.0.15
  - jupyter_pygment   : 0.3.0
  - jupyter_server   : 2.27.3 üåê
  - jupyterlab       : 4.4.3 üñ•Ô∏è

## Dependencias üì¶
Para el correcto funcionamiento del programa se necesitan las siguientes dependencias:

- chromadb=1.0.9
- langchain=0.3.25
- langchain-chroma=0.2.4
- langchain-community=0.3.24
- langchain-huggingface=0.2.0
- langchain-text-splitters=0.3.8
- loguru=0.7.3
- rich=14.0.0 üé®
- pypdf=5.5.0
- sentence-transformers=4.1.0
- streamlit=1.45.1
- langchain-ollama=0.3.3 Ô∏è

Para m√°s informaci√≥n se puede consultar el documento [requirements.yml](requirements.yml) üìÑ

---

## Integrantes
- [fwgalde](https://github.com/fwgalde)
- [EARSV](https://github.com/EARSV)

## Notas üìù
- Se utilizaron LLM‚Äôs ü§ñ para la realizaci√≥n de documentaci√≥n y formato del c√≥digo. As√≠ como la realizaci√≥n de esta documentaci√≥n tipo Di√°taxis.
- La aplicaci√≥n para Hugginface fue realizada con el SDK de Gradio y se puede ver el dise√±o de la "interfaz" en el archivo [app.py](app.py).