# -*- coding: utf-8 -*-
"""6_transformers_con_hugginface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jPb-m6fkHJvy7Gv04oU7urSVfWWnO0Jx
"""

!pip install -U fsspec

!pip install -U datasets

pip install evaluate

"""- Selecciona un modelo pre-entrenado como base y realiza *fine-tuning* para resolver alguna tarea de NLP que te parezca reelevante
  - Procura utilizar datasets pequeños para que sea viable
  - Recuerda las posibles tareas disponibles en HF `*For<task>`
- Desarrolla y pon en producción un prototipo del modelo
  - Incluye una URL pública donde podamos ver tu proyecto
  - Recomendamos usar framewoks de prototipado (*streamlit* o *gradio*) y el *free-tier* de *spaces* de hugging face
    - https://huggingface.co/spaces/launch
    - https://huggingface.co/docs/hub/spaces-sdks-streamlit
    - https://huggingface.co/docs/hub/spaces-sdks-gradio
- Reporta que tan bien se resolvió la tarea y que tan útil fue tu app
- Reporta retos y dificultades al realizar el *fine-tuning* y al poner tu modelo en producción

## Extra

- Utiliza [code carbon](https://codecarbon.io/#howitwork) para reportar las emisiones de tu app
"""

from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast
from transformers import Trainer, TrainingArguments
from datasets import load_dataset
import evaluate
import numpy as np

# Cargar dataset y tokenizer
dataset = load_dataset('imdb')
small_train = dataset['train'].shuffle(seed=42).select(range(1000))
small_test = dataset['test'].shuffle(seed=42).select(range(200))

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Usando dispositivo: {device}")

# Tokenización
# Esta función toma un diccionario de ejemplos (donde 'text' es la clave de los textos de entrada)
# y aplica el tokenizador para convertir cada texto en una secuencia de tokens entendible por el modelo.
# Se utiliza padding para que todas las secuencias tengan la misma longitud (max_length),
# y truncation para cortar los textos que excedan la longitud máxima permitida por el modelo.
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)

# Se aplica la función de tokenización a los datasets de entrenamiento y prueba.
# El parámetro batched=True permite procesar múltiples ejemplos a la vez, lo cual acelera el procesamiento.
train_dataset = small_train.map(tokenize_function, batched=True)
test_dataset = small_test.map(tokenize_function, batched=True)

# Métrica
# Se carga la métrica de precisión (accuracy) utilizando la biblioteca `evaluate` de Hugging Face.
metric = evaluate.load("accuracy")

# Esta función se usa para calcular métricas de evaluación durante el entrenamiento.
# Toma las predicciones del modelo (logits) y las etiquetas verdaderas,
# convierte los logits en etiquetas predichas usando argmax, y luego calcula la precisión.
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Modelo
# Se carga un modelo preentrenado de clasificación de texto basado en DistilBERT.
# 'distilbert-base-uncased' es una versión ligera de BERT que no distingue entre mayúsculas y minúsculas.
# El parámetro num_labels=2 indica que es una tarea de clasificación binaria.
# Finalmente, el modelo se mueve al dispositivo adecuado (CPU o GPU) para entrenamiento o inferencia.
model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=2
).to(device)  # Mover modelo a GPU

training_args = TrainingArguments(
    output_dir='./results',
    eval_strategy='epoch',
    save_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=16,  # Ajustar según memoria GPU
    per_device_eval_batch_size=32,   # Puede ser mayor para evaluación
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    report_to="none",
    fp16=True,  # Activar mixed-precision para mayor velocidad en GPUs modernas
    gradient_accumulation_steps=2,  # Útil si el batch size máximo es pequeño
)

# 6. Entrenador
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# Fine-tuning
trainer.train()

model.save_pretrained('./sentiment_model')
tokenizer.save_pretrained('./sentiment_model')

# Verificar que existen
!ls ./sentiment_model

from google.colab import files
!zip -r sentiment_model.zip sentiment_model/
files.download('sentiment_model.zip')