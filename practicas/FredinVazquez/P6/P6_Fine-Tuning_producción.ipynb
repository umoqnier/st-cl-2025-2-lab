{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794d5120",
   "metadata": {},
   "source": [
    "# Vázquez Martínez Fredin Alberto\n",
    "\n",
    "## Práctica 6: Fine-tuning en producción\n",
    "\n",
    "### **Fecha de entrega: 11 de Mayo de 2025 11:59pm**\n",
    "\n",
    "1. Selecciona un modelo pre-entrenado como base y realiza fine-tuning para resolver alguna tarea de NLP que te parezca reelevante\n",
    "    * Procura utilizar datasets pequeños para que sea viable\n",
    "    * Recuerda las posibles tareas disponibles en HF *For<task>\n",
    "\n",
    "## Desarrollo\n",
    "\n",
    "Lo primero a realizar es la selección de los datos, el dataset a elegir será sobre phishing, el dataset está compuesto por diferentes sub datasets. El dataset a usar es sobre mensajes de texto, el objetivo será poder distinguir si es spam, Smishing o Ham.\n",
    "\n",
    "**La explicación es:**\n",
    "\n",
    "Este conjunto de datos contiene 5,971 mensajes de texto (SMS) clasificados en tres categorías:\n",
    "\n",
    "1. Spam (489 mensajes): Publicidad no deseada, promociones engañosas o mensajes comerciales no solicitados.\n",
    "\n",
    "2. Smishing (638 mensajes): Mensajes fraudulentos que intentan robar información personal (como contraseñas o datos bancarios) mediante enlaces o engaños.\n",
    "\n",
    "3. Ham (4,844 mensajes): Mensajes legítimos y seguros (conversaciones normales, alertas válidas, etc.).\n",
    "\n",
    "**En cuestiones del dataset tenemos las siguientes etiquetas**\n",
    "\n",
    "* 1 (Phishing/Atacante): Incluye spam + smishing (1,127 mensajes)\n",
    "\n",
    "* 0 (Benigno/Inofensivo): Solo ham (4,844 mensajes)\n",
    "\n",
    "Se hará un fine tuning usando el transformer BERT para poder clasificar estos mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f142f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documentos - Archivos\\Octavo semestre\\DP\\p10\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Documentos - Archivos\\Octavo semestre\\DP\\p10\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Librerias generales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerias de transformers\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from transformers import TrainingArguments\n",
    "from transformers import TFAutoModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c05cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"roberta-base\"\n",
    "model_path = \"google-bert/bert-base-cased\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee812eb",
   "metadata": {},
   "source": [
    "### **División de datos**\n",
    "\n",
    "Realizamos la creación del dataset a un formato aceptable por el transformer, en este caso usamos DatasetDict.\n",
    "\n",
    "Se decidió crear 3 conjuntos de datos diferentes:\n",
    "\n",
    "* Train: usado para el fine tuning\n",
    "* Validation: será usado para poder probar durante entrenamiento, no se usa test porque puede existir riesgo de datos filtrados.\n",
    "* Test: una vez entrenado el modelo, se prueba con estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43ea353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>re : 6 . 1100 , disc : uniformitarianism , re ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the other side of * galicismos * * galicismo *...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re : equistar deal tickets are you still avail...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nHello I am your hot lil horny toy.\\n    I am...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>software at incredibly low prices ( 86 % lower...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20132</th>\n",
       "      <td>You have won a Nokia 7250i. This is what you g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20133</th>\n",
       "      <td>Get ur 1st RINGTONE FREE NOW! Reply to this ms...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20134</th>\n",
       "      <td>Ur cash-balance is currently 500 pounds - to m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20135</th>\n",
       "      <td>Records indicate you were involved in an accid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20136</th>\n",
       "      <td>call now 08707509020 Just 20p per min NTT Ltd...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20137 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      re : 6 . 1100 , disc : uniformitarianism , re ...      0\n",
       "1      the other side of * galicismos * * galicismo *...      0\n",
       "2      re : equistar deal tickets are you still avail...      0\n",
       "3      \\nHello I am your hot lil horny toy.\\n    I am...      1\n",
       "4      software at incredibly low prices ( 86 % lower...      1\n",
       "...                                                  ...    ...\n",
       "20132  You have won a Nokia 7250i. This is what you g...      1\n",
       "20133  Get ur 1st RINGTONE FREE NOW! Reply to this ms...      1\n",
       "20134  Ur cash-balance is currently 500 pounds - to m...      1\n",
       "20135  Records indicate you were involved in an accid...      1\n",
       "20136   call now 08707509020 Just 20p per min NTT Ltd...      1\n",
       "\n",
       "[20137 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 14095\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3021\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3021\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "## Obteniendo el dataset ##\n",
    "dataset = load_dataset(\"ealvaradob/phishing-dataset\", \"texts\", trust_remote_code=True)\n",
    "\n",
    "## Obteniendo datos y etiquetas ##\n",
    "dataset = dataset['train'].to_pandas()\n",
    "display(dataset)\n",
    "\n",
    "data = dataset['text'].values\n",
    "labels = dataset['label'].values\n",
    "\n",
    "\n",
    "## Haciendo la división entre entrenamiento y testing ##\n",
    "train_data, testvalid_data, train_labels, testvalid_labels = train_test_split(\n",
    "    data, labels, test_size=0.3, random_state=42 , stratify=labels\n",
    ")\n",
    "\n",
    "# División de test+valid en test y valid\n",
    "test_data, valid_data, test_labels, valid_labels = train_test_split(\n",
    "    testvalid_data, testvalid_labels, test_size=0.5, random_state=42 , stratify=testvalid_labels\n",
    ")\n",
    "\n",
    "\n",
    "# Convertir a DatasetDict de Hugging Face\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict({'text': train_data, 'label': train_labels}),\n",
    "    'test': Dataset.from_dict({'text': test_data, 'label': test_labels}),\n",
    "    'valid': Dataset.from_dict({'text': valid_data, 'label': valid_labels}),\n",
    "})\n",
    "\n",
    "display(train_test_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e80d5b",
   "metadata": {},
   "source": [
    "### **Tokenizador**\n",
    "Usando el tokenizador de BERT para poder tokenizar todo el conjunto, así mismo debido a la longitud variada de cada dato en el dataset original, se hará un padding para que todo quede del mismo tamaño. Así mismo, será necesario truncar en caso de alcanzar la longitud máxima por defecto, son 512 tokens, y también vamos a pedir que los tensores se regresen formato de pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1289efe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14095/14095 [00:20<00:00, 703.18 examples/s] \n",
      "Map: 100%|██████████| 3021/3021 [00:01<00:00, 2002.29 examples/s]\n",
      "Map: 100%|██████████| 3021/3021 [00:01<00:00, 2342.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## TOKENIZACION ##\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch, padding=True, truncation=True, return_tensors='pt') # se desactiva el truncado para evitar perder informacion\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenize(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "# Ahora ya podemos hacer la tokenizacion\n",
    "train_encodings = train_test_valid_dataset['train'].map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "val_encodings = train_test_valid_dataset['valid'].map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "test_encodings = train_test_valid_dataset['test'].map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e4bf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 14095\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 3021\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 3021\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_encodings)\n",
    "display(val_encodings)\n",
    "display(test_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d47df",
   "metadata": {},
   "source": [
    "### **Resolviendo el balance de clases**\n",
    "\n",
    "Podemos ver que es muy claro la diferencia de clases entre mensajes de phishing con mensajes normales, para poder resolver eso se propone colocar pesos a cada clase, esto nos ayudará a reducir el riesgo de tener un sobreajuste, o sea evitar que todo lo clasifique como mensaje normal, lo cual para la mayoría de veces será cierto, pero sea incapaz de diferenciar entre uno de phishing y uno normal por la cantidad de registros que tenemos por clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de2ac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80773639, 1.31238361])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Extraer etiquetas de 'train' dataset\n",
    "y_train = np.array(train_encodings[\"label\"])\n",
    "\n",
    "# Calcular pesos de las clases\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),  # Proporcionar clases únicas explícitamente\n",
    "    y=y_train                   # Etiquetas reales del dataset\n",
    ")\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91952fd",
   "metadata": {},
   "source": [
    "### **Entrenamiento**\n",
    "\n",
    "Antes de realizar el entrenamiento, para seguir ayudando al desbalanceo de clases se decidió cambiar la función de pérdida, por una específica para lidiar con estos tipos de problemas de dataset no balanceados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f77652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "# Agregando un scheduler para reducir  la tasa de aprendizaje \n",
    "\n",
    "class CustomTrainer(Trainer):   # Focal loss para clases desbalanceadas\n",
    "    def create_scheduler(self, num_training_steps: int, optimizer):\n",
    "        \"\"\"\n",
    "        Crea un planificador personalizado de tasa de aprendizaje (learning rate scheduler)\n",
    "        que reduce la tasa de aprendizaje cada 5 épocas (epochs), multiplicándola por 0.9.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_training_steps : int\n",
    "            Número total de pasos de entrenamiento (batches) a ejecutar en todas las épocas.\n",
    "\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            Optimizador al cual se le aplicará el planificador de tasa de aprendizaje.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.lr_scheduler.LambdaLR\n",
    "            Instancia del planificador de tasa de aprendizaje que reduce el learning rate\n",
    "            cada 5 épocas completas de entrenamiento.\n",
    "        \"\"\"\n",
    "\n",
    "        def lr_lambda(current_step):\n",
    "            # Reduce LR every 5 epochs\n",
    "            if current_step > 0 and (current_step // steps_per_epoch) % 5 == 0:\n",
    "                return 0.9 ** ((current_step // steps_per_epoch) // 5)\n",
    "            return 1.0\n",
    "\n",
    "        # Get the number of steps per epoch\n",
    "        global steps_per_epoch\n",
    "        steps_per_epoch = num_training_steps // self.args.num_train_epochs\n",
    "        \n",
    "        self.lr_scheduler = LambdaLR(self.optimizer, lr_lambda)\n",
    "        return self.lr_scheduler\n",
    "\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Calcula una función de pérdida ponderada usando CrossEntropyLoss para clases desbalanceadas.\n",
    "        Se espera que haya dos clases, por lo que se fija `num_labels=2`. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : PreTrainedModel\n",
    "            Modelo de Hugging Face que produce logits al realizar una pasada hacia adelante.\n",
    "\n",
    "        inputs : dict\n",
    "            Diccionario que contiene al menos `input_ids`, `attention_mask`, y `labels`.\n",
    "\n",
    "        return_outputs : bool, optional\n",
    "            Si es True, devuelve tanto la pérdida como la salida del modelo. Por defecto es False.\n",
    "\n",
    "        kwargs : dict\n",
    "            Parámetros adicionales pasados al modelo durante la inferencia.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor or Tuple[torch.Tensor, ModelOutput]\n",
    "            La pérdida escalar si `return_outputs=False`; si es True, devuelve una tupla\n",
    "            `(loss, outputs)` donde `outputs` es el resultado del modelo.\n",
    "        \"\"\"\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        model.config.num_labels = 2\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcula métricas de evaluación para clasificación binaria, incluyendo precisión,\n",
    "    recall, f1-score y exactitud (accuracy), usando un promedio micro.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eval_pred : Tuple[np.ndarray, np.ndarray]\n",
    "        Tupla que contiene:\n",
    "        - predictions: matriz de logits o probabilidades predichas por el modelo.\n",
    "        - labels: etiquetas verdaderas del conjunto de evaluación.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Diccionario con la métrica f1-score con promedio micro.\n",
    "        Además, imprime un reporte de clasificación detallado con etiquetas \"Benign\" y \"Phishing\".\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=-1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"micro\")\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    print(\"\\nClassification Report:\\n\", classification_report(\n",
    "        labels, preds, target_names=[\"Benign\", \"Phishing\"]))\n",
    "    \n",
    "    return {\"f1-score\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0c999",
   "metadata": {},
   "source": [
    "### **Fine-Tuning de Bert Base Case**\n",
    "\n",
    "Al final, se observó tras experimentación que no era necesario hacer tantas épocas para este problema, con 5 y la configuración anterior, fue suficiente para lograr resultados deseables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0cd3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_33180\\3768385150.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "d:\\Documentos - Archivos\\Octavo semestre\\DP\\p10\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4405' max='4405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4405/4405 44:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.082858</td>\n",
       "      <td>0.981794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.118153</td>\n",
       "      <td>0.977160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.101646</td>\n",
       "      <td>0.985435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.114599</td>\n",
       "      <td>0.986428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.173086</td>\n",
       "      <td>0.976829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.99      0.99      0.99      1870\n",
      "    Phishing       0.98      0.98      0.98      1151\n",
      "\n",
      "    accuracy                           0.98      3021\n",
      "   macro avg       0.98      0.98      0.98      3021\n",
      "weighted avg       0.98      0.98      0.98      3021\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.98      0.99      0.98      1870\n",
      "    Phishing       0.98      0.96      0.97      1151\n",
      "\n",
      "    accuracy                           0.98      3021\n",
      "   macro avg       0.98      0.97      0.98      3021\n",
      "weighted avg       0.98      0.98      0.98      3021\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.99      0.99      0.99      1870\n",
      "    Phishing       0.98      0.98      0.98      1151\n",
      "\n",
      "    accuracy                           0.99      3021\n",
      "   macro avg       0.98      0.98      0.98      3021\n",
      "weighted avg       0.99      0.99      0.99      3021\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.99      0.99      0.99      1870\n",
      "    Phishing       0.98      0.98      0.98      1151\n",
      "\n",
      "    accuracy                           0.99      3021\n",
      "   macro avg       0.99      0.99      0.99      3021\n",
      "weighted avg       0.99      0.99      0.99      3021\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.97      0.99      0.98      1870\n",
      "    Phishing       0.99      0.95      0.97      1151\n",
      "\n",
      "    accuracy                           0.98      3021\n",
      "   macro avg       0.98      0.97      0.98      3021\n",
      "weighted avg       0.98      0.98      0.98      3021\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4405, training_loss=0.051801630208494986, metrics={'train_runtime': 2669.3247, 'train_samples_per_second': 26.402, 'train_steps_per_second': 1.65, 'total_flos': 1.8542751626496e+16, 'train_loss': 0.051801630208494986, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "# Tenemos antagonista, protagonista e inocente\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2) # cargando el modelo preentrenado para la tarea de clasificacion\n",
    "\n",
    "task = 'PhishingClassification'\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_path}-finetuned-{task}\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-8,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=1e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1-score',\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_encodings,\n",
    "    eval_dataset=val_encodings,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(AdamW(model.parameters(), 2e-5), None),\n",
    "    # callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccd82b",
   "metadata": {},
   "source": [
    "### **Subiendo el modelo a mi cuenta de hugging face**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c458ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0836234ef2471e92775d562a792146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86867687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 433M/433M [07:36<00:00, 950kB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Fredin14/bert-base-cased-finetuned-PhishingClassificationl/commit/1cb2a439f8e2b7fd881e1ccdc2f94a5113aef55f', commit_message='Upload BertForSequenceClassification', commit_description='', oid='1cb2a439f8e2b7fd881e1ccdc2f94a5113aef55f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Fredin14/bert-base-cased-finetuned-PhishingClassificationl', endpoint='https://huggingface.co', repo_type='model', repo_id='Fredin14/bert-base-cased-finetuned-PhishingClassificationl'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"Fredin14/bert-base-cased-finetuned-PhishingClassificationl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f9952",
   "metadata": {},
   "source": [
    "### **Reporta que tan bien se resolvió la tarea y que tan útil fue tu app**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546117de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.99      0.98      0.99      1870\n",
      "    Phishing       0.97      0.98      0.98      1151\n",
      "\n",
      "    accuracy                           0.98      3021\n",
      "   macro avg       0.98      0.98      0.98      3021\n",
      "weighted avg       0.98      0.98      0.98      3021\n",
      "\n",
      "Métricas en el conjunto de test: {'test_loss': 0.1090025082230568, 'test_f1-score': 0.9831181727904668, 'test_runtime': 33.3854, 'test_samples_per_second': 90.489, 'test_steps_per_second': 5.661}\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.predict(test_encodings)\n",
    "\n",
    "# Obtener las métricas de evaluación\n",
    "test_metrics = test_results.metrics\n",
    "print(f\"Métricas en el conjunto de test: {test_metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b77323",
   "metadata": {},
   "source": [
    "En los resultados obtenidos, se pueden ver que se obtuvo buenas resultados en general. Para todas las métricas, incluyendo precisión, recall y f1 score se puede ver que los resultados fueron muy aceptables.\n",
    "\n",
    "Finalmente, la accuracy que se obtiene es muy bueno, siendo 98%. \n",
    "\n",
    "Los resultados son en general muy buenos, aún cuando no tenemos datos balanceados, logramos obtener un buen rendimiento para la distinguir entre mensajes de texto que pueden ser phishing y mensajes que no tienen una carga maliciosa.\n",
    "\n",
    "La aplicación realizada es útil para una detección de posibles mensajes que pueden ser de phishing. Este modelo puede servir como base para hacer un segundo fine-tuning sobre otro conjunto de datos, y así potenciar la tarea. Sin embargo, también se debe de considerar que el transformer se le realizó un fine-tuning sobre este conjunto de datos, lo cual en caso de tener otro patrón para crear mensajes de phishing, tendríamos que ver si el transformer es capaz de poder deducir esos nuevos patrones.\n",
    "\n",
    "Los resultados obtenidos fueron tan buenos por la cantidad de configuraciones realizadas. Considerando que le dimos diferentes pesos a cada clase, esto al transformer le es de mucha ayuda para no crear sesgos ya que en caso de tener más datos de una clase que otra, podríamos en efecto tener un accuracy alto, pero es porque la mayoría de registros están etiquetados con la misma clase. Pero, podemos ver que para este caso la ayuda de los pesos para cada clase además del learning rate nos permitió cumplir con la tarea para cada clase. Y no tenemos sesgos por lo reportado en los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e57c9",
   "metadata": {},
   "source": [
    "### **Retos y dificultades al realizar el fine-tuning y al poner tu modelo en producción**\n",
    "\n",
    "El principal reto fue lidiar con el conjunto de datos debido a la cantidad de datos para cada clase, sin embargo fue de mucha ayuda tener un learning rate bajo además de poder usar el peso para cada clase.\n",
    "\n",
    "Así mismo, el tiempo del fine tuning, al ser una cantidad relativamente pequeña, alrededor de 5k registros en el dataset, para el trabajo de computo que involucra, es algo considerable. No obstante, para mi caso usando mi GPU me tardó alrededor de 2 horas, obteniendo resultados deseables.\n",
    "\n",
    "Por esa parte, ya tenía experiencia trabajando con datos no balanceados, entonces ya sabía las diferentes técnicas a usar para tratar datasets con estos tipos de problemas, por lo cual realmente no me llevó gran problema abordar este dataset. \n",
    "\n",
    "\n",
    "**¿Fue necesario un preprocesamiento?**\n",
    "\n",
    "Realmente no, en el dataset se recomendaba trabajar con los datos tal cual se daban, la razón de estos es el que el uso de diferentes stop words, así mismo como las conjugaciones de los verbos, así más valioso conservar esa información que quitar las stop words o lematizar. Ya que el BERT puede sacar y aprovechar esa información para notar ciertos patrones que ayuden a la clasificación. Es por eso que no se realizó ningún preprocesamiento.\n",
    "\n",
    "**¿Dificultad al ponerlo en producción?**\n",
    "\n",
    "Solo revisando la documentación fue intuitivo y divertido usar los espacios de hugging face para compartir estos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df6965",
   "metadata": {},
   "source": [
    "### **Prototipo del modelo en producción**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2acb7",
   "metadata": {},
   "source": [
    "Se hizo uso del framework sdks-streamlit para crear la aplicación y compartir el proyecto.\n",
    "\n",
    "**Aquí se puede encontrar el link público del proyecto:** https://huggingface.co/spaces/Fredin14/Phishing_Detection_BERT \n",
    "\n",
    "Para esto lo primero fue subir el modelo a mi cuenta de hugging face, ya una vez teniendo el modelo en la nube, se puede manipular para poder incluirlo usando pipeline. Posteriormente se crea un espacio en hugging face spaces, para colocar ahí la aplicación. Lo demás es solamente configurar la interfaz gráfica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1e838",
   "metadata": {},
   "source": [
    "### **Probando el modelo cargado desde hugging face**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")  # tokenizador de bert\n",
    "pipe = pipeline(\"text-classification\", model=\"Fredin14/bert-base-cased-finetuned-PhishingClassificationl\", tokenizer=tokenizer,  padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d00f8bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1870\n",
      "           1       0.97      0.98      0.98      1151\n",
      "\n",
      "    accuracy                           0.98      3021\n",
      "   macro avg       0.98      0.98      0.98      3021\n",
      "weighted avg       0.98      0.98      0.98      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts_list = test_data.tolist()\n",
    "test_results = pipe(texts_list)\n",
    "\n",
    "y_pred = []\n",
    "for pred in test_results:\n",
    "    y_pred.append(int(pred['label'][-1]))\n",
    "\n",
    "print(classification_report(test_labels,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee260e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
