# -*- coding: utf-8 -*-
"""Copia de 4_preprocess_lm_neuronal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K3QGMBA7qhy0VIyK7ZhkMC6dUGh2GwFo

# 4. Preprocesamiento y Modelos del Lenguaje (Neuronal)

<img src="https://2.bp.blogspot.com/-oDvCIkIjwXw/VdWWxfvmq3I/AAAAAAAARUE/r0MrmbNzMz8/s1600/inputoutput.jpg" width=500>

## Objetivos

- Aplicar preprocesamiento a corpus en espa√±ol e ingl√©s
- Entender el funcionamiento de algoritmos de sub-word tokenization
  - Aplicar BPE a corpus
- Entrenar un modelo del lenguaje neuronal con la arquitectura de Bengio

## ¬øQu√© es una palabra?

- Tecnicas de procesamiento del lenguaje depende de las palabras y las oraciones.
  - Debemos identificar estos elementos para poder procesarlos
- Este paso de identificaci√≥n de palabras y oraciones se le llama segmentaci√≥n de texto o **tokenizaci√≥n** (*tokenization*)
- Adem√°s de la identificaci√≥n de unidades aplicaremos transformaciones al texto

Aunque la definici√≥n de lo que es una palabra puede parecer obvia es tremendamente dif√≠cil.

- I'm
- we'd
- I've
- Diego's Bicycle

## Elementos del preprocesamiento

- Normalizaci√≥n
    - Pasar todo a min√∫sculas
    - Pasar texto a cierta norma ortogr√°fica
- Quitar stopwords
- Quitar elementos de marcado (HTML, XML)
- Tokenizaci√≥n

### Stopwords
"""

import re
import nltk
from nltk.corpus import stopwords
from rich import print as rprint

BASE_PATH = "drive/MyDrive"
CORPORA_PATH = f"{BASE_PATH}/corpora/tokenization"
MODELS_PATH = f"{BASE_PATH}/models/sub-word"

nltk.download('stopwords')

rprint(stopwords.words("spanish")[:15])

"""### Normalizaci√≥n

<center><img src="https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fimg1.wikia.nocookie.net%2F__cb20140504152558%2Fspongebob%2Fimages%2Fe%2Fe3%2FThe_spongebob.jpg&f=1&nofb=1&ipt=28368023b54a7c84c9100025981b1042d0f4ca3ceaac53be42094cc1c3794348&ipo=images" height=300 width=300></center>
"""

import unicodedata

def strip_accents(s: str) -> str:
   return ''.join(
       c for c in unicodedata.normalize('NFD', s)
       if unicodedata.category(c) != 'Mn'
   )

strip_accents("mam√° hoy quer√≠a que me oigan en el ol√≠mpo")

"""- https://www.unicode.org/reports/tr44/#GC_Values_Table

> And keep in mind, these manipulations may significantly alter the meaning of the text. Accents, Umlauts etc. are not "decoration".
- [oefe](https://stackoverflow.com/users/49793/oefe) - [source](https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-normalize-in-a-python-unicode-string)
"""

def preprocess(words: list[str], regex: str="\w+", lang: str="en") -> list[str]:
    """Preprocess step for corpus

    Parameters
    ----------
    words: list[str]
        Words of a given corpus
    regex: str
        Optional regex to filter patterns in words. Default \w+
    lang: str
        Optional lang for choice stopwords. Default "en"

    Return
    ------
    list:
        List of words filtered and normalized

    """
    stop_lang = "english" if lang=="en" else "spanish"
    result = []
    for word in words:
        word = re.sub(f"[^\w\s]", "", word).lower()
        if word.isalpha():
            result.append(word)
    return result

"""#### ¬øPara otras lenguas?

- No hay muchos recursos :(
- Pero para el nahuatl esta `pyelotl` :)

#### Normalizando el Nahuatl
"""

!pip install elotl

import elotl.corpus
import elotl.nahuatl.orthography

axolotl = elotl.corpus.load("axolotl")

# Tres posibles normalizadores: sep, inali, ack
# Sauce: https://pypi.org/project/elotl/

nahuatl_normalizer = elotl.nahuatl.orthography.Normalizer("sep")

axolotl[1][1]

nahuatl_normalizer.normalize(axolotl[1][1])

nahuatl_normalizer.to_phones(axolotl[1][1])

"""## Tokenizaci√≥n

### Word-base tokenization
"""

text = """
¬°¬°¬°Mam√° prendele a la grabadora!!!, ¬øllamaste a las vecinas? Corre la voz porque, efectivamente, !estoy en la tele! üì∫
"""

text.split()

# [a-zA-Z_]
regex = r"\w+"
re.findall(regex, text)

re.findall(regex, "El valor de PI es 3.14159")

"""<img src="http://images.wikia.com/battlebears/images/2/2c/Troll_Problem.jpg" with="250" height="250">

- Vocabularios gigantescos dif√≠ciles de procesar
- Generalmente, entre m√°s grande es el vocabulario m√°s pesado ser√° nuestro modelo

**Ejemplo:**
- Si queremos representaciones vectoriales de nuestras palabras obtendr√≠amos vectores distintos para palabras similares
    - ni√±o = `v1(39, 34, 5,...)`
    - ni√±os = `v2(9, 4, 0,...)`
    - ni√±a = `v3(2, 1, 1,...)`
    - ...
- Tendr√≠amos tokens con baj√≠sima frecuencia
    - merequetengue = `vn(0,0,1,...)`

### Una soluci√≥n: Steaming/Lematizaci√≥n (AKA la vieja confiable)

![](https://i.pinimg.com/736x/77/df/89/77df89e6ff57d332ba4e5d7bff723133--meme.jpg)
"""

from nltk.corpus import brown
nltk.download('brown')

brown_corpus = preprocess(brown.words()[:100000])
rprint(brown_corpus[0])

rprint(brown_corpus[:10])

from collections import Counter

rprint(f"[yellow]Brown Vanilla")
rprint("Tokens:", len(brown.words()))
rprint("Tipos:", len(Counter(brown.words())))

rprint(f"[green]Brown Preprocess")
rprint("Tokens:", len(brown_corpus))
rprint("Tipos:", len(Counter(brown_corpus)))

"""#### Steamming"""

from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer("english")

stemmed_brown = [stemmer.stem(word) for word in brown_corpus]

"""#### Lematizaci√≥n"""

!python -m spacy download en_core_web_md
!python -m spacy download es_core_news_md

import spacy

def lemmatize(words: list, lang: str="en") -> list:
    model = "en_core_web_md" if lang == "en" else "es_core_news_md"
    nlp = spacy.load(model)
    nlp.max_length = 1500000
    lemmatizer = nlp.get_pipe("lemmatizer")
    return [token.lemma_ for token in nlp(" ".join(words))]

lemmatized_brown = lemmatize(brown_corpus)

from rich.panel import Panel

rprint("Tipos ([blue]word-based):", len(Counter(brown_corpus)))
rprint("Tipos ([yellow]Steamming):", len(Counter(stemmed_brown)))
rprint("Tipos ([green]Lemmatized):", len(Counter(lemmatized_brown)))

"""#### More problems?

<img src="https://uploads.dailydot.com/2019/10/Untitled_Goose_Game_Honk.jpeg?auto=compress%2Cformat&ixlib=php-3.3.0" width="250" height="250">

- M√©todos dependientes de las lenguas
- Se pierde informaci√≥n
- Ruled-based

## Subword-tokenization salva el d√≠a ü¶∏üèº‚Äç‚ôÄÔ∏è

![](https://gifdb.com/images/high/super-cow-and-chicken-daxvak1q16quwd9p.webp)

- Segmentaci√≥n de palabras en unidades m√°s peque√±as (*sub-words*)
- Obtenemos tipos menos variados y con mayores frecuencias
    - Esto le gusta modelos basados en m√©todos estad√≠sticos
- Palabras frecuentes no deber√≠an separarse
- Palabras largas y raras deber√≠a descomponerse en sub-palabras significativas
- Los m√©todos estadisticos que no requieren conocimiento a priori de las lenguas
"""

text = "Let's do tokenization!"
result = ["Let's", "do", "token", "ization", "!"]
print(f"Objetivo: {text} -> {result}")

"""### Algoritmos

Existen varios algoritmos para hacer *subword-tokenization* como los que se listan a continuaci√≥n:

- Byte-Pair Encoding (BPE)
- WordPiece
- Unigram

#### BPE

- Segmenmentaci√≥n iterativa, comienza segmentando en secuencias de caracteres
- Junta los pares m√°s frecuentes (*merge operation*)
- Termina cuando se llega al n√∫mero de *merge operations* especificado o n√∫mero de vocabulario deseado (*hyperparams*, depende de la implementaci√≥n)
- Introducido en el paper: [Neural Machine Translation of Rare Words with Subword Units, (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%HTML
# <iframe width="960" height="515" src="https://www.youtube.com/embed/HEikzVL-lZU"></iframe>

!pip install transformers

SENTENCE = "Let's do this tokenization to enable hypermodernization on my tokens tokenized üëÅÔ∏èüëÅÔ∏èüëÅÔ∏è!!!"

from transformers import GPT2Tokenizer

bpe_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
rprint(bpe_tokenizer.tokenize(SENTENCE))

encoded_tokens = bpe_tokenizer(SENTENCE)
rprint(encoded_tokens["input_ids"])

rprint(bpe_tokenizer.decode(encoded_tokens["input_ids"]))

"""- En realidad GPT-2 usa *Byte-Level BPE*
    - Evitamos vocabularios de inicio grandes (Ej: unicode)
    - Usamos bytes como vocabulario base
    - Evitamos *Out Of Vocabulary, OOV* (aka `[UKW]`)

#### WordPiece

- Descrito en el paper: [Japanese and Korean voice search, (Schuster et al., 2012) ](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)
- Similar a BPE, inicia el vocabulario con todos los caracteres y aprende los merges
- En contraste con BPE, no elige con base en los pares m√°s frecuentes si no los pares que maximicen la probabilidad de aparecer en los datos una vez que se agregan al vocabulario

$$score(a_i,b_j) = \frac{f(a_i,b_j)}{f(a_i)f(b_j)}$$

- Esto quiere decir que evalua la perdida de realizar un *merge* asegurandoce que vale la pena hacerlo

- Algoritmo usado en `BERT`
"""

# Commented out IPython magic to ensure Python compatibility.
# %%HTML
# <iframe width="960" height="500" src="https://www.youtube.com/embed/qpv6ms_t_1A"></iframe>

from transformers import BertTokenizer
SENTENCE = "üåΩ" + SENTENCE + "üî•"
wp_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
rprint(wp_tokenizer.tokenize(SENTENCE))

"""<center><img src="https://us-tuna-sounds-images.voicemod.net/9cf541d2-dd7f-4c1c-ae37-8bc671c855fe-1665957161744.jpg"></center>"""

rprint(wp_tokenizer(SENTENCE))

"""#### Unigram

- Algoritmo de subpword tokenization introducido en el paper: [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf)
- En contraste con BPE o WordPiece, este algoritmo inicia con un vocabulario muy grande y va reduciendolo hasta llegar tener un vocabulario deseado
- En cada iteraci√≥n se calcula la perdida de quitar cierto elemento del vocabulario
    - Se quitar√° `p%` elementos que menos aumenten la perdida en esa iteraci√≥n
- El algoritmo termina cuando se alcanza el tama√±o deseado del vocabulario

Sin embargo, *Unigram* no se usa por si mismo en algun modelo de Hugging Face:
> "Unigram is not used directly for any of the models in the transformers, but it‚Äôs used in conjunction with SentencePiece." - Hugging face guy

#### SentencePiece

- No asume que las palabras estan divididas por espacios
- Trata la entrada de texto como un *stream* de datos crudos. Esto incluye al espacio como un caract√©r a usar
- Utiliza BPE o Unigram para construir el vocabulario
"""

# https://github.com/google/sentencepiece#installation
!pip install sentencepiece

from transformers import XLNetTokenizer

tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased")
rprint(tokenizer.tokenize(SENTENCE))

"""#### Objetivo de los subword tokenizers

- Buscamos que modelos de redes neuronales tenga datos mas frecuentes
- Esto ayuda a que en principio "aprendan" mejor
- Reducir el numero de tipos
- Reducir el numero de OOV

### Vamos a tokenizar üåà
![](https://i.pinimg.com/736x/58/6b/88/586b8825f010ce0e3f9c831f568aafa8.jpg)

#### Corpus en espa√±ol: CESS
"""

nltk.download("cess_esp")

from nltk.corpus import cess_esp

cess_words = cess_esp.words()

" ".join(cess_words[:30])

cess_plain_text = " ".join(preprocess(cess_words))

rprint(f"'{cess_plain_text[300:600]}'")

cess_preprocessed_words = cess_plain_text.split()

with open(f"{CORPORA_PATH}/cess_plain.txt", "w") as f:
    f.write(cess_plain_text)

"""#### Corpus Ingl√©s: Gutenberg"""

nltk.download('gutenberg')
nltk.download("punkt_tab")

from nltk.corpus import gutenberg

gutenberg_words = gutenberg.words()[:200000]

rprint(" ".join(gutenberg_words[:30]))

gutenberg_plain_text = " ".join(preprocess(gutenberg_words))

rprint(gutenberg_plain_text[:100])

gutenberg_preprocessed_words = gutenberg_plain_text.split()

with open(f"{CORPORA_PATH}/gutenberg_plain.txt", "w") as f:
    f.write(gutenberg_plain_text)

"""#### Tokenizando el espa√±ol con Hugging face"""

from transformers import AutoTokenizer

spanish_tokenizer = AutoTokenizer.from_pretrained("dccuchile/bert-base-spanish-wwm-uncased")
rprint(spanish_tokenizer.tokenize(cess_plain_text[1000:1400]))

cess_types = Counter(cess_words)

rprint(cess_types.most_common(10))

cess_tokenized = spanish_tokenizer.tokenize(cess_plain_text)
rprint(cess_tokenized[:10])
cess_tokenized_types = Counter(cess_tokenized)

rprint(cess_tokenized_types.most_common(30))

cess_lemmatized_types = Counter(lemmatize(cess_words, lang="es"))

rprint(cess_lemmatized_types.most_common(30))

rprint("CESS")
rprint(f"Tipos ([blue]word-base): {len(cess_types)}")
rprint(f"Tipos ([yellow]lemmatized): {len(cess_lemmatized_types)}")
rprint(f"Tipos ([green]sub-word): {len(cess_tokenized_types)}")

"""#### Tokenizando para el ingl√©s"""

gutenberg_types = Counter(gutenberg_words)

gutenberg_tokenized = wp_tokenizer.tokenize(gutenberg_plain_text)
gutenberg_tokenized_types = Counter(gutenberg_tokenized)

rprint(gutenberg_tokenized_types.most_common(100))

gutenberg_lemmatized_types = Counter(lemmatize(gutenberg_preprocessed_words))

rprint(gutenberg_lemmatized_types.most_common(20))

rprint("Gutenberg")
rprint(f"Tipos ([blue]word-base): {len(gutenberg_types)}")
rprint(f"Tipos ([yellow]lemmatized): {len(gutenberg_lemmatized_types)}")
rprint(f"Tipos ([green]sub-word): {len(gutenberg_tokenized_types)}")

"""#### OOV: out of vocabulary

Palabras que se vieron en el entrenamiento pero no estan en el test
"""

from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(gutenberg_words, test_size=0.3, random_state=42)
rprint(len(train_data), len(test_data))

s_1 = {"a", "b", "c", "d", "e"}
s_2 = {"a", "x", "y", "d"}
rprint(s_1 - s_2)
rprint(s_2 - s_1)

oov_test = set(test_data) - set(train_data)

for word in list(oov_test)[:3]:
    rprint(f"{word} in train: {word in set(train_data)}")

train_tokenized, test_tokenized = train_test_split(gutenberg_tokenized, test_size=0.3, random_state=42)
rprint(len(train_tokenized), len(test_tokenized))

oov_tokenized_test = set(test_tokenized) - set(train_tokenized)

rprint("OOV ([yellow]word-base):", len(oov_test))
rprint("OOV ([green]sub-word):", len(oov_tokenized_test))

"""## Entrenando nuestro modelo con BPE
![](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmedia1.tenor.com%2Fimages%2Fd565618bb1217a7c435579d9172270d0%2Ftenor.gif%3Fitemid%3D3379322&f=1&nofb=1&ipt=9719714edb643995ce9d978c8bab77f5310204960093070e37e183d5372096d9&ipo=images)
"""

!pip install subword-nmt

!ls {CORPORA_PATH}

!cat {CORPORA_PATH}/gutenberg_plain.txt

!subword-nmt learn-bpe -s 300 < \
 {CORPORA_PATH}/gutenberg_plain.txt > \
  {MODELS_PATH}/gutenberg.model

!echo "I need to process this sentence because tokenization can be useful" \
| subword-nmt apply-bpe -c {MODELS_PATH}/gutenberg.model

!subword-nmt learn-bpe -s 1500 < \
{CORPORA_PATH}/gutenberg_plain.txt > \
 {MODELS_PATH}/gutenberg_high.model

!echo "I need to process this sentence because tokenization can be useful" \
| subword-nmt apply-bpe -c {MODELS_PATH}/gutenberg_high.model

"""### Aplicandolo a otros corpus: La biblia üìñüáªüá¶"""

BIBLE_FILE_NAMES = {"spa": "spa-x-bible-reinavaleracontemporanea", "eng": "eng-x-bible-kingjames"}

import requests

def get_bible_corpus(lang: str) -> str:
    """Download bible file corpus from GitHub repo"""
    file_name = BIBLE_FILE_NAMES[lang]
    r = requests.get(f"https://raw.githubusercontent.com/ximenina/theturningpoint/main/Detailed/corpora/corpusPBC/{file_name}.txt.clean.txt")
    return r.text

def write_plain_text_corpus(raw_text: str, file_name: str) -> None:
    """Write file text on disk"""
    with open(f"{file_name}.txt", "w") as f:
        f.write(raw_text)

"""#### Biblia en Ingl√©s"""

eng_bible_plain_text = get_bible_corpus("eng")
eng_bible_words = eng_bible_plain_text.lower().replace("\n", " ").split()

print(eng_bible_words[:10])

len(eng_bible_words)

eng_bible_types = Counter(eng_bible_words)

rprint(eng_bible_types.most_common(30))

eng_bible_lemmas_types = Counter(lemmatize(eng_bible_words, lang="en"))

write_plain_text_corpus(eng_bible_plain_text, f"{CORPORA_PATH}/eng-bible")

!subword-nmt apply-bpe -c {MODELS_PATH}/gutenberg_high.model < \
 {CORPORA_PATH}/eng-bible.txt > \
 {CORPORA_PATH}/eng-bible-tokenized.txt

with open(f"{CORPORA_PATH}/eng-bible-tokenized.txt", 'r') as f:
    tokenized_data = f.read()
eng_bible_tokenized = tokenized_data.split()

rprint(eng_bible_tokenized[:10])

len(eng_bible_tokenized)

eng_bible_tokenized_types = Counter(eng_bible_tokenized)
len(eng_bible_tokenized_types)

eng_bible_tokenized_types.most_common(30)

"""#### ¬øQu√© pasa si aplicamos el modelo aprendido con Gutenberg a otras lenguas?"""

spa_bible_plain_text = get_bible_corpus('spa')
spa_bible_words = spa_bible_plain_text.replace("\n", " ").lower().split()

spa_bible_words[:10]

len(spa_bible_words)

spa_bible_types = Counter(spa_bible_words)
len(spa_bible_types)

spa_bible_types.most_common(30)

spa_bible_lemmas_types = Counter(lemmatize(spa_bible_words, lang="es"))
len(spa_bible_lemmas_types)

write_plain_text_corpus(spa_bible_plain_text, f"{CORPORA_PATH}/spa-bible")

!subword-nmt apply-bpe -c {MODELS_PATH}/gutenberg_high.model < \
 {CORPORA_PATH}/spa-bible.txt > \
 {CORPORA_PATH}/spa-bible-tokenized.txt

with open(f"{CORPORA_PATH}/spa-bible-tokenized.txt", "r") as f:
    tokenized_text = f.read()
spa_bible_tokenized = tokenized_text.split()

spa_bible_tokenized[:10]

len(spa_bible_tokenized)

spa_bible_tokenized_types = Counter(spa_bible_tokenized)
len(spa_bible_tokenized_types)

spa_bible_tokenized_types.most_common(40)

"""### Type-token Ratio (TTR)

- Una forma de medir la variazi√≥n del vocabulario en un corpus
- Este se calcula como $TTR = \frac{len(types)}{len(tokens)}$
- Puede ser √∫til para monitorear la variaci√≥n lexica de un texto
"""

rprint("Informaci√≥n de la biblia en Ingl√©s")
rprint("Tokens:", len(eng_bible_words))
rprint("Types ([blue]word-base):", len(eng_bible_types))
rprint("Types ([yellow]lemmatized)", len(eng_bible_lemmas_types))
rprint("Types ([green]BPE):", len(eng_bible_tokenized_types))
rprint("TTR ([blue]word-base):", len(eng_bible_types)/len(eng_bible_words))
rprint("TTR ([green]BPE):", len(eng_bible_tokenized_types)/len(eng_bible_tokenized))

rprint("Bible Spanish Information")
rprint("Tokens:", len(spa_bible_words))
rprint("Types ([blue]word-base):", len(spa_bible_types))
rprint("Types ([yellow]lemmatized)", len(spa_bible_lemmas_types))
rprint("Types ([green]BPE):", len(spa_bible_tokenized_types))
rprint("TTR ([blue]word-base):", len(spa_bible_types)/len(spa_bible_words))
rprint("TTR ([green]BPE):", len(spa_bible_tokenized_types)/len(spa_bible_tokenized))

"""## Modelos del Lenguaje Neuronales (Bengio)

- [(Bengio et al 2003)](https://dl.acm.org/doi/10.5555/944919.944966) proponen una arquitecura neuronal como alternativa a los modelos del lenguaje estad√≠sticos
- Esta arquitectura lidia mejor con los casos donde las probabilidades se hacen cero, sin necesidad de aplicar una t√©cnica de smoothing.

<p float="left">
  <img src="https://toppng.com/public/uploads/preview/at-the-movies-will-smith-meme-tada-11562851401lnexjqtwf9.png" width="100" />
  <img src="https://abhinavcreed13.github.io/assets/images/bengio-model.png" width="600"/>
</p>
"""

nltk.download('reuters')
nltk.download('punkt_tab')
nltk.download('stopwords')

from nltk.corpus import reuters, stopwords
from nltk import ngrams

def preprocess_corpus(corpus: list[str]) -> list[str]:
    """Funci√≥n de preprocesamiento

    Esta funci√≥n est√° dise√±ada para preprocesar
    corpus para modelos del lenguaje neuronales.
    Agrega tokens de inicio y fin, normaliza
    palabras a minusculas
    """
    preprocessed_corpus = []
    for sent in corpus:
        result = [word.lower() for word in sent]
        # Al final de la oraci√≥n
        result.append("<EOS>")
        result.insert(0, "<BOS>")
        preprocessed_corpus.append(result)
    return preprocessed_corpus

def get_words_freqs(corpus: list[list[str]]):
    """Calcula la frecuencia de las palabras en un corpus"""
    words_freqs = {}
    for sentence in corpus:
        for word in sentence:
            words_freqs[word] = words_freqs.get(word, 0) + 1
    return words_freqs

UNK_LABEL = "<UNK>"
def get_words_indexes(words_freqs: dict) -> dict:
    """Calcula los indices de las palabras dadas sus frecuencias"""
    result = {}
    for idx, word in enumerate(words_freqs.keys()):
        # Happax legomena happends
        if words_freqs[word] == 1:
            # Temp index for unknowns
            result[UNK_LABEL] = len(words_freqs)
        else:
            result[word] = idx

    return {word: idx for idx, word in enumerate(result.keys())}, {idx: word for idx, word in enumerate(result.keys())}

corpus = preprocess_corpus(reuters.sents())

len(corpus)

words_freqs = get_words_freqs(corpus)

words_freqs["the"]

len(words_freqs)

count = 0
for word, freq in words_freqs.items():
    if freq == 1 and count <= 10:
        print(word, freq)
        count += 1

words_indexes, index_to_word = get_words_indexes(words_freqs)

words_indexes["the"]

index_to_word[16]

len(words_indexes)

len(index_to_word)

def get_word_id(words_indexes: dict, word: str) -> int:
    """Obtiene el id de una palabra dada

    Si no se encuentra la palabra se regresa el id
    del token UNK
    """
    unk_word_id = words_indexes[UNK_LABEL]
    return words_indexes.get(word, unk_word_id)

"""### Obtenemos trigramas

Convertiremos los trigramas obtenidos a secuencias de idx, y preparamos el conjunto de entrenamiento $x$ y $y$

- x: Contexto
- y: Predicci√≥n de la siguiente palabra
"""

def get_train_test_data(corpus: list[list[str]], words_indexes: dict, n: int) -> tuple[list, list]:
    """Obtiene el conjunto de train y test

    Requerido en el step de entrenamiento del modelo neuronal
    """
    x_train = []
    y_train = []
    for sent in corpus:
        n_grams = ngrams(sent, n)
        for w1, w2, w3 in n_grams:
            x_train.append([get_word_id(words_indexes, w1), get_word_id(words_indexes, w2)])
            y_train.append([get_word_id(words_indexes, w3)])
    return x_train, y_train

"""### Preparando Pytorch

$x' = e(x_1) \oplus e(x_2)$

$h = \tanh(W_1 x' + b)$

$y = softmax(W_2 h)$
"""

# cargamos bibliotecas
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from google.colab import drive
import time
drive.mount('/content/drive', force_remount=True)

# Setup de parametros
EMBEDDING_DIM = 200
CONTEXT_SIZE = 2
BATCH_SIZE = 256
H = 100
torch.manual_seed(42)
# Tama√±o del Vocabulario
V = len(words_indexes)

x_train, y_train = get_train_test_data(corpus, words_indexes, n=3)

import numpy as np

train_set = np.concatenate((x_train, y_train), axis=1)
# partimos los datos de entrada en batches
train_loader = DataLoader(train_set, batch_size = BATCH_SIZE)

"""### Creamos la arquitectura del modelo"""

# Trigram Neural Network Model
class TrigramModel(nn.Module):
    """Clase padre: https://pytorch.org/docs/stable/generated/torch.nn.Module.html"""

    def __init__(self, vocab_size, embedding_dim, context_size, h):
        super(TrigramModel, self).__init__()
        self.context_size = context_size
        self.embedding_dim = embedding_dim
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear1 = nn.Linear(context_size * embedding_dim, h)
        self.linear2 = nn.Linear(h, vocab_size)

    def forward(self, inputs):
        # x': concatenation of x1 and x2 embeddings   -->
        # self.embeddings regresa un vector por cada uno de los √≠ndices que se les pase como entrada.
        # view() les cambia el tama√±o para concatenarlos
        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))
        # h: tanh(W_1.x' + b)  -->
        out = torch.tanh(self.linear1(embeds))
        # W_2.h                 -->
        out = self.linear2(out)
        # log_softmax(W_2.h)      -->
        # dim=1 para que opere sobre renglones, pues al usar batchs tenemos varios vectores de salida
        log_probs = F.log_softmax(out, dim=1)

        return log_probs

# Seleccionar la GPU si est√° disponible
device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"

#torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Training on device {device}")

# 1. P√©rdida. Negative log-likelihood loss
loss_function = nn.NLLLoss()

# 2. Instanciar el modelo y enviarlo a device
model = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H).to(device)

# 3. Optimizaci√≥n. ADAM optimizer
optimizer = optim.Adam(model.parameters(), lr = 2e-3)

# ------------------------- TRAIN & SAVE MODEL ------------------------
EPOCHS = 10
for epoch in range(EPOCHS):
    st = time.time()
    print("\n--- Training model Epoch: {} ---".format(epoch))
    for it, data_tensor in enumerate(train_loader):
        # Mover los datos a la GPU
        context_tensor = data_tensor[:,0:2].to(device)
        target_tensor = data_tensor[:,2].to(device)

        model.zero_grad()

        # FORWARD:
        log_probs = model(context_tensor)

        # compute loss function
        loss = loss_function(log_probs, target_tensor)

        # BACKWARD:
        loss.backward()
        optimizer.step()

        if it % 500 == 0:
            print("Training Iteration {} of epoch {} complete. Loss: {}; Time taken (s): {}".format(it, epoch, loss.item(), (time.time()-st)))
            st = time.time()

    # saving model
    model_path = f'drive/MyDrive/LM_neuronal/model_{device}_context_{CONTEXT_SIZE}_epoch_{epoch}.dat'
    torch.save(model.state_dict(), model_path)
    print(f"Model saved for epoch={epoch} at {model_path}")

def get_model(path: str) -> TrigramModel:
    """Obtiene modelo de pytorch desde disco"""
    model_loaded = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)
    model_loaded.load_state_dict(torch.load(path))
    model_loaded.to(device)
    model_loaded.eval()
    return model_loaded

PATH = "drive/MyDrive/LM_neuronal/model_cpu_context_2_epoch_9.dat"
model = get_model(PATH)

#model = get_model(PATH)
W1 = "<BOS>"
W2 = "my"

IDX1 = get_word_id(words_indexes, W1)
IDX2 = get_word_id(words_indexes, W2)

#Obtenemos Log probabidades p(W3|W2,W1)
probs = model(torch.tensor([[IDX1,  IDX2]]).to(device)).detach().tolist()

len(probs[0])

# Creamos diccionario con {idx: logprob}
model_probs = {}
for idx, p in enumerate(probs[0]):
  model_probs[idx] = p

# Sort:
model_probs_sorted = sorted(((prob, idx) for idx, prob in model_probs.items()), reverse=True)

# Printing word  and prob (retrieving the idx):
topcandidates = 0
for prob, idx in model_probs_sorted:
  #Retrieve the word associated with that idx
  word = index_to_word[idx]
  print(idx, word, prob)

  topcandidates += 1

  if topcandidates > 10:
    break

print(index_to_word.get(model_probs_sorted[0][1]))

"""### Generacion de lenguaje"""

def get_likely_words(model: TrigramModel, context: str, words_indexes: dict, index_to_word: dict, top_count: int=10) -> list[tuple]:
    model_probs = {}
    words = context.split()
    idx_word_1 = get_word_id(words_indexes, words[0])
    idx_word_2 = get_word_id(words_indexes, words[1])
    probs = model(torch.tensor([[idx_word_1, idx_word_2]]).to(device)).detach().tolist()

    for idx, p in enumerate(probs[0]):
        model_probs[idx] = p

    # Strategy: Sort and get top-K words to generate text
    return sorted(((prob, index_to_word[idx]) for idx, prob in model_probs.items()), reverse=True)[:top_count]

sentence = "this is"
get_likely_words(model, sentence, words_indexes, index_to_word, 3)

from random import randint

def get_next_word(words: list[tuple[float, str]]) -> str:
    # From a top-K list of words get a random word
    return words[randint(0, len(words)-1)][1]

get_next_word(get_likely_words(model, sentence, words_indexes, index_to_word))

MAX_TOKENS = 50
TOP_COUNT = 10
def generate_text(model: TrigramModel, history: str, words_indexes: dict, index_to_word: dict, tokens_count: int=0) -> None:
    next_word = get_next_word(get_likely_words(model, history, words_indexes, index_to_word, top_count=TOP_COUNT))
    print(next_word, end=" ")
    tokens_count += 1
    if tokens_count == MAX_TOKENS or next_word == "<EOS>":
        return
    generate_text(model, history.split()[1]+ " " + next_word, words_indexes, index_to_word, tokens_count)

sentence = "mexico is"
print(sentence, end=" ")
generate_text(model, sentence, words_indexes, index_to_word)

"""# Pr√°ctica 4: Modelos del Lenguaje Neuronales

**Fecha de entrega: 6 de abril de 2025 11:59pm**

A partir del modelo entrenado:

- Sacar los embeddings de las palabras del vocabulario

- Visualizar en 2D los embeddings de algunas palabras (quiz√° las m√°s frecuentes, excluyendo stopwords)

- Seleccione algunas palabras y verifique s√≠ realmente codifican nociones sem√°nticas, e,g, similitud sem√°ntica con similitud coseno entre dos vectores, analog√≠as por medios de operaciones de vectores
"""

import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

## Tokens m√°s frecuentes con longitud mayor a 5
freqs = pd.Series({k:v for k,v in words_freqs.items() if k.isalpha() and k not in stopwords.words('english') and len(k) > 5})
freqs = freqs[(freqs.index != "<BOS>") & (freqs.index != "<EOS>") & (freqs.index != "<UNK>")]
freqs.sort_values(ascending=False).head(10)

subseted_words = list(freqs.index[:100])
ids_subseted_words = [get_word_id(words_indexes, word) for word in subseted_words]
embeddings = model.embeddings(torch.tensor(ids_subseted_words)).detach().numpy()

reducer = PCA(n_components=2)
embeddings_2d = reducer.fit_transform(embeddings)

fig = plt.figure(figsize=(10,10))
ax = fig.add_subplot(111)
ax.scatter(embeddings_2d[:,0], embeddings_2d[:,1])
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
ax.set_title("Embeddings de palabras")

for i, word in enumerate(subseted_words):
    ax.annotate(word, (embeddings_2d[i,0], embeddings_2d[i,1]))
plt.show()

"""Podemos observar que si bien ciertas palabras relacionadas entre s√≠ aparecen juntas como los son _worried_ y _problems_, _imports_ y _exports_, _electric_ y _electronics_; en general la represenetaci√≥n no es muy buena ya qye gran parte de los embedings se distribuyen uniformemente al rededor de un c√∫mulo central, esto puede deberse a que el corpus de entrenamiento es relativamente peque√±o y no muy diverso ya que se centra en noticias.

Ejemplos de similitud l√©xica con distancia coseno
"""

def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

cosine_similarity(model.embeddings(torch.tensor(get_word_id(words_indexes, "king"))).detach().numpy(),
                  model.embeddings(torch.tensor(get_word_id(words_indexes, "queen"))).detach().numpy())

cosine_similarity(model.embeddings(torch.tensor(get_word_id(words_indexes, "worried"))).detach().numpy(),
                  model.embeddings(torch.tensor(get_word_id(words_indexes, "problems"))).detach().numpy())

cosine_similarity(model.embeddings(torch.tensor(get_word_id(words_indexes, "money"))).detach().numpy(),
                  model.embeddings(torch.tensor(get_word_id(words_indexes, "dollars"))).detach().numpy())

cosine_similarity(model.embeddings(torch.tensor(get_word_id(words_indexes, "bussines"))).detach().numpy(),
                  model.embeddings(torch.tensor(get_word_id(words_indexes, "company"))).detach().numpy())

cosine_similarity(model.embeddings(torch.tensor(get_word_id(words_indexes, "nations"))).detach().numpy(),
                  model.embeddings(torch.tensor(get_word_id(words_indexes, "countries"))).detach().numpy())

cosine_similarity(model.embeddings(torch.tensor(get_word_id(words_indexes, "foreigner"))).detach().numpy(),
                  model.embeddings(torch.tensor(get_word_id(words_indexes, "inmigrant"))).detach().numpy())

cosine_similarity(model.embeddings(torch.tensor(get_word_id(words_indexes, "stock"))).detach().numpy(),
                  model.embeddings(torch.tensor(get_word_id(words_indexes, "asset"))).detach().numpy())

cosine_similarity(model.embeddings(torch.tensor(get_word_id(words_indexes, "good"))).detach().numpy(),
                  model.embeddings(torch.tensor(get_word_id(words_indexes, "bad"))).detach().numpy())

cosine_similarity(model.embeddings(torch.tensor(get_word_id(words_indexes, "imports"))).detach().numpy(),
                  model.embeddings(torch.tensor(get_word_id(words_indexes, "exports"))).detach().numpy())

"""De la misma manera que en la representaci√≥n gr√°fica, la similitud entre los embedings de palabras relacionadas, en general si presentan una similitud mayor a cero, pero no es el la similitud que se esperar√≠a de sin√≥nimos, sin embargo un caso curioso es el de _foreigner_ e _inmigrant_ que tienen una similitud casi de 1 lo que indica que ambos t√©rminos son utilizados como palabras equivalentes en el corpus de noticias."""

def find_analogies(word1, word2, word3):
    """
    Encuentra una analog√≠a del tipo "word1 es a word2 como word3 es a X".
    Utiliza embeddings de palabras para calcular la relaci√≥n vectorial y encontrar
    la palabra m√°s similar que complete la analog√≠a.

    Args:
        word1 (str): Primera palabra de la relaci√≥n
        word2 (str): Segunda palabra de la relaci√≥n
        word3 (str): Tercera palabra que inicia la segunda parte de la relaci√≥n

    Returns:
        str: La palabra que mejor completa la analog√≠a "word1:word2 :: word3:?"

    Ejemplo:
        >>> find_analogies("rey", "hombre", "reina")
        "mujer"
    """
    # Convertir todas las palabras a min√∫sculas para normalizaci√≥n
    word1, word2, word3 = word1.lower(), word2.lower(), word3.lower()

    # Obtener los √≠ndices de las palabras en el vocabulario
    idx1 = get_word_id(words_indexes, word1)
    idx2 = get_word_id(words_indexes, word2)
    idx3 = get_word_id(words_indexes, word3)

    # Obtener los embeddings de las palabras (vectores num√©ricos)
    emb1, emb2, emb3 = model.embeddings(torch.tensor([idx1, idx2, idx3])).detach().numpy()

    # Calcular el vector prototipo para la analog√≠a: (word2 - word1) + word3
    prototype = (emb2 - emb1) + emb3
    # Normalizar el vector prototipo
    prototype = prototype / np.linalg.norm(prototype)

    # Buscar la palabra m√°s cercana en el espacio de embeddings
    closest_word = []
    min_cosine_dist = -2  # Inicializar con valor m√≠nimo posible (-1 ser√≠a el m√≠nimo real)

    for i in range(len(index_to_word)):
        # Saltar las palabras de entrada para no devolverlas como resultado
        if index_to_word[i] in {word1, word2, word3}:
            continue

        # Obtener embedding de la palabra candidata
        emb = model.embeddings(torch.tensor([i])).detach().numpy().flatten()
        # Calcular similitud coseno con el prototipo
        sim = cosine_similarity(prototype, emb)

        # Actualizar la palabra m√°s cercana si encontramos mayor similitud
        if sim > min_cosine_dist:
            min_cosine_dist = sim
            closest_word = index_to_word[i]

    return closest_word

find_analogies("king", "man", "queen")

find_analogies("stock", "asset", "money")

find_analogies("imports", "buy", "exports")

find_analogies("mexico", "latin", "america")

find_analogies("industry", "manufacturing", "banking")

find_analogies("rich", "development", "poor")

"""## Referencias

- [Language models - Lena Voita](https://lena-voita.github.io/nlp_course/language_modeling.html#generation_strategies)
- [A Neural Probabilistic Model - Bengio](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- Parte del c√≥digo de esta pr√°ctica fue retomado del trabajo de la Dr. Ximena Guitierrez Vasques
"""