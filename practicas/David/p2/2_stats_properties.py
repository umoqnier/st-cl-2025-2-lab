# -*- coding: utf-8 -*-
"""Copia de 2_stats_properties.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D3flsjfqxxFYytFgBax4YJKKrsgeNcuV

# 2. Propiedades estad√≠sticas del lenguaje

## Objetivos

- Mostrar el uso de CFG y derivados
    - Ejemplos de parseo de dependencias
- Ejemplificar etiquetado NER usando bibliotecas existentes
- Explorar propiedades estad√≠sticas del lenguaje natural y observar los siguientes fenomenos:
    - La distribuci√≥n de Zipf
    - La distribuci√≥n de Heap

- Implementar bolsas de palabras
    - Aplicar *TF.IDF*

## Perspectivas formales

- Fueron el primer acercamiento al procesamiento del lenguaje natural. Sin embargo tienen varias **desventajas**
- Requieren **conocimiento previo de la lengua**
- Las herramientas son especificas de la lengua
- Los fenomenos que se presentan son muy amplios y dif√≠cilmente se pueden abarcar con reglas formales (muchos casos especiales)
- Las reglas tienden a ser rigidas y no admiten incertidumbre en el resultado

### Sintaxis

![](https://imgs.xkcd.com/comics/formal_languages_2x.png)

**[audience looks around] 'What just happened?' 'There must be some context we're missing.'**

#### Parsing basado en reglas

- Gramaticas libres de contexto:

$G = (T, N, O, R)$
* $T$ s√≠mbolos terminales.
* $N$ s√≠mbolos no terminales.
* $O$ simbolo inicial o nodo ra√≠z.
* $R$ reglas de la forma $X \longrightarrow \gamma$ donde $X$ es no terminal y $\gamma$ es una secuencia de terminales y no terminales
"""

import nltk

plain_grammar = """
S -> NP VP
NP -> Det N | Det N PP | 'I'
VP -> V NP | VP PP
PP -> P NP
Det -> 'an' | 'my'
N -> 'elephant' | 'pajamas'
V -> 'shot'
P -> 'in'
"""

grammar = nltk.CFG.fromstring(plain_grammar)
# Cambiar analizador y trace
analyzer = nltk.ChartParser(grammar)

sentence = "I shot an elephant in my pajamas".split()
trees = analyzer.parse(sentence)

for tree in trees:
    print(tree, type(tree))
    print('\nBosquejo del √°rbol:\n')
    print(tree.pretty_print(unicodelines=True, nodedist=1))

"""## Perspectiva estad√≠stica

- Puede integrar aspectos de la perspectiva formal
- Lidia mejor con la incertidumbre y es menos rigida que la perspectiva formal
- No requiere conocimiento profundo de la lengua. Se pueden obtener soluciones de forma no supervisada

## Modelos estad√≠sticos

- Las **frecuencias** juegan un papel fundamental para hacer una descripci√≥n acertada del lenguaje
- Las frecuencias nos dan informaci√≥n de la **distribuci√≥n de tokens**, de la cual podemos estimar probabilidades.
- Existen **leyes emp√≠ricas del lenguaje** que nos indican como se comportan las lenguas a niveles estad√≠sticos
- A partir de estas leyes y otras reglas estad√≠sticas podemos crear **modelos del lenguaje**; es decir, asignar probabilidades a las unidades ling√º√≠sticas

### Probabilistic Context Free Grammar
"""

taco_grammar = nltk.PCFG.fromstring("""
O    -> FN FV     [0.7]
O    -> FV FN     [0.3]
FN   -> Sust      [0.6]
FN   -> Det Sust  [0.4]
FV   -> V FN      [0.8]
FV   -> FN V      [0.2]
Sust -> 'Juan'    [0.5]
Sust -> 'tacos'   [0.5]
Det  -> 'unos'    [1.0]
V    -> 'come'    [1.0]
""")
viterbi_parser = nltk.ViterbiParser(taco_grammar)

sentences = [
    "Juan come unos tacos",
    "unos tacos Juan come"
]
for sent in sentences:
    for tree in viterbi_parser.parse(sent.split()):
        print(tree)
        print("Versi√≥n bosque")
        tree.pretty_print(unicodelines=True, nodedist=1)

"""### Parseo de dependencias

Un parseo de dependencias devuelve las dependencias que se dan entre los tokens de una oraci√≥n. Estas dependencias suelen darse entre pares de tokens. Esto es, que relaciones tienen las palabras con otras palabras.

##### Freeling - https://nlp.lsi.upc.edu/freeling/demo/demo.php
"""

import spacy
from spacy import displacy

!python -m spacy download es_core_news_md

nlp = spacy.load("es_core_news_md")

doc = nlp("La ni√±a come un suani")

displacy.render(doc, style="dep")

for chunk in doc.noun_chunks:
    print("text::", chunk.text)
    print("root::", chunk.root.text)
    print("root dep::", chunk.root.dep_)
    print("root head::", chunk.root.head.text)
    print("="*10)

for token in doc:
    print("token::", token.text)
    print("dep::", token.dep_)
    print("head::", token.head.text)
    print("head POS::", token.head.pos_)
    print("CHILDS")
    print([child for child in token.children])
    print("="*10)

"""#### Named Entity Recognition (NER)

El etiquetado NER consiste en identificar "objetos de la vida real" como organizaciones, paises, personas, entre otras y asignarles su etiqueta correspondiente. Esta tarea es del tipo *sequence labeling* ya que dado un texto de entrada el modelo debe identificar los intervalos del texto y etiquetarlos adecuadamente con la entidad que le corresponde. Veremos un ejemplo a continuaci√≥n.
"""

!pip install datasets

from datasets import load_dataset

data = load_dataset("alexfabbri/multi_news")

# Explorar data
data?

!python -m spacy download en_core_web_md

nlp = spacy.load("en_core_web_md")

import random

random.seed(42)
corpus = random.choices(data["train"]["summary"], k=3)
docs = list(nlp.pipe(corpus))
for j, doc in enumerate(docs):
    print(f"DOC #{j+1}")
    doc.user_data["title"] = " ".join(doc.text.split()[:10])
    for i, ent in enumerate(doc.ents):
        print(" -"*10, f"Entity #{i}")
        print(f"\tTexto={ent.text}")
        print(f"\tstart/end={ent.start_char}-{ent.end_char}")
        print(f"\tLabel={ent.label_}")

displacy.render(docs, style="ent")

spacy.explain("NORP")

"""[Available labels](https://spacy.io/models/en)

## Leyes estad√≠sticas
"""

# Bibliotecas
from collections import Counter
import matplotlib.pyplot as plt
#plt.rcParams['figure.figsize'] = [10, 6]
import numpy as np
import pandas as pd

mini_corpus = """Humanismo es un concepto polis√©mico que se aplica tanto al estudio de las letras humanas, los
estudios cl√°sicos y la filolog√≠a grecorromana como a una gen√©rica doctrina o actitud vital que
concibe de forma integrada los valores humanos. Por otro lado, tambi√©n se denomina humanis-
mo al ¬´sistema de creencias centrado en el principio de que las necesidades de la sensibilidad
y de la inteligencia humana pueden satisfacerse sin tener que aceptar la existencia de Dios
y la predicaci√≥n de las religiones¬ª, lo que se aproxima al laicismo o a posturas secularistas.
Se aplica como denominaci√≥n a distintas corrientes filos√≥ficas, aunque de forma particular,
al humanismo renacentista1 (la corriente cultural europea desarrollada de forma paralela al
Renacimiento a partir de sus or√≠genes en la Italia del siglo XV), caracterizado a la vez por su
vocaci√≥n filol√≥gica cl√°sica y por su antropocentrismo frente al teocentrismo medieval
"""
words = mini_corpus.replace("\n", " ").split(" ")
len(words)

vocabulary = Counter(words)
vocabulary.most_common(10)

len(vocabulary)

def get_frequencies(vocabulary: Counter, n: int) -> list:
    return [_[1] for _ in vocabulary.most_common(n)]

def plot_frequencies(frequencies: list, title="Freq of words", log_scale=False):
    x = list(range(1, len(frequencies)+1))
    plt.plot(x, frequencies, "-v")
    plt.xlabel("Freq rank (r)")
    plt.ylabel("Freq (f)")
    if log_scale:
        plt.xscale("log")
        plt.yscale("log")
    plt.title(title)

frequencies = get_frequencies(vocabulary, 100)
plot_frequencies(frequencies)

plot_frequencies(frequencies, log_scale=True)

"""**¬øQu√© pasar√° con m√°s datos? üìä**

### Ley Zipf

Exploraremos el Corpus de Referencia del Espa√±ol Actual [CREA](https://www.rae.es/banco-de-datos/crea/crea-anotado)
"""

corpus_freqs = pd.read_csv("crea_frecs.txt", sep=" ")

corpus_freqs.head(15)

corpus_freqs.iloc[0]

corpus_freqs[corpus_freqs["word"] == "barriga"]

corpus_freqs["freq"].plot(marker="o")
plt.title('Ley de Zipf en el CREA')
plt.xlabel('rank')
plt.ylabel('freq')
plt.show()

corpus_freqs['freq'].plot(loglog=True, legend=False)
plt.title('Ley de Zipf en el CREA (log-log)')
plt.xlabel('log rank')
plt.ylabel('log frecuencia')
plt.show()

"""- Notamos que las frecuencias entre lenguas siguen un patr√≥n
- Pocas palabras (tipos) son muy frecuentes, mientras que la mayor√≠a de palabras ocurren pocas veces

De hecho, la frecuencia de la palabra que ocupa la posici√≥n r en el rank, es proporcional a $\frac{1}{r}$ (La palabra m√°s frecuente ocurrir√° aproximadamente el doble de veces que la segunda palabra m√°s frecuente en el corpus y tres veces m√°s que la tercer palabra m√°s frecuente del corpus, etc)

$$f(w_r) \propto \frac{1}{r^Œ±}$$

Donde:
- $r$ es el rank que ocupa la palabra en el corpus
- $f(w_r)$ es la frecuencia de la palabra en el corpus
- $\alpha$ es un par√°metro, el valor depender√° del corpus o fen√≥meno que estemos observando

#### Formulaci√≥n de la Ley de Zipf:

$f(w_{r})=\frac{c}{r^{\alpha }}$

En la escala logar√≠timica:

$log(f(w_{r}))=log(\frac{c}{r^{\alpha }})$

$log(f(w_{r}))=log (c)-\alpha log (r)$

#### ‚ùì ¬øC√≥mo estimar el par√°metro $\alpha$?

Podemos hacer una regresi√≥n lineal minimizando la suma de los errores cuadr√°ticos:

$J_{MSE}=\sum_{r}^{}(log(f(w_{r}))-(log(c)-\alpha log(r)))^{2}$
"""

from scipy.optimize import minimize

ranks = np.array(corpus_freqs.index) + 1
frecs = np.array(corpus_freqs['freq'])

# Inicializaci√≥n
a0 = 1

# Funci√≥n de minimizaci√≥n:
func = lambda a: sum((np.log(frecs)-(np.log(frecs[0])-a*np.log(ranks)))**2)

# Apliando minimos cuadrados
a_hat = minimize(func, a0).x[0]

print('alpha:', a_hat, '\nMSE:', func(a_hat))

def plot_generate_zipf(alpha: np.float64, ranks: np.array, freqs: np.array) -> None:
    plt.plot(np.log(ranks),  np.log(freqs[0]) - alpha*np.log(ranks), color='r', label='Aproximaci√≥n Zipf')

plot_generate_zipf(a_hat, ranks, frecs)
plt.plot(np.log(ranks),np.log(frecs), color='b', label='Distribuci√≥n CREA')
plt.xlabel('log ranks')
plt.ylabel('log frecs')
plt.legend(bbox_to_anchor=(1, 1))
plt.show()

"""### Ley de Heap

Relaci√≥n entre el n√∫mero de **tokens** y **tipos** de un corpus

$$T \propto N^b$$

D√≥nde:

- $T = $ n√∫mero de tipos
- $N = $ n√∫mero de tokens
- $b = $ par√°metro

- **TOKENS**: N√∫mero total de palabras dentro del texto (incluidas repeticiones)
- **TIPOS**: N√∫mero total de palabras √∫nicas en el texto

#### üìä Ejercicio: Muestra el plot de tokens vs types para el corpus CREA

**HINT:** Obtener tipos y tokens acumulados
"""

# PLOT tokens vs types
total_tokens = corpus_freqs["freq"].sum()
total_types = len(corpus_freqs)

corpus_sorted = corpus_freqs.sort_values(by="freq", ascending=False)
corpus_sorted["cum_tokens"] = corpus_sorted["freq"].cumsum()
corpus_sorted["cum_types"] = range(1, total_types +1)

# Plot de la ley de Heap
plt.plot(corpus_sorted['cum_types'], corpus_sorted['cum_tokens'])
plt.xscale("log")
plt.yscale("log")
plt.xlabel('Types')
plt.ylabel('Tokens')
plt.title('Ley de Heap')
plt.show()

"""### Representaciones vectoriales est√°ticas (est√°ticos)

- Buscamos una forma de mapear textos al **espacio vectorial**. Tener una representaci√≥n numerica permite su procesamiento.
    - Similitud de docs
    - Clasificacion (agrupamiento)
- Veremos el enfoque de la Bolsa de Palabras (Bag of Words)
   - Matriz de documentos-terminos
   - Cada fila es un vector con $N$ features donde las features ser√°n el vocabulario del corpus

<center>
<img src="https://preview.redd.it/sqkqsuit7o831.jpg?width=1024&auto=webp&s=2d18d38fe9d04a4a62c9a889e7b34ef14b425630" width=500></center>
"""

import gensim

doc_1 = "Augusta Ada King, condesa de Lovelace (Londres, 10 de diciembre de 1815-√≠d., 27 de noviembre de 1852), registrada al nacer como Augusta Ada Byron y conocida habitualmente como Ada Lovelace, fue una matem√°tica y escritora brit√°nica, c√©lebre sobre todo por su trabajo acerca de la computadora mec√°nica de uso general de Charles Babbage, la denominada m√°quina anal√≠tica. Fue la primera en reconocer que la m√°quina ten√≠a aplicaciones m√°s all√° del c√°lculo puro y en haber publicado lo que se reconoce hoy como el primer algoritmo destinado a ser procesado por una m√°quina, por lo que se le considera como la primera programadora de ordenadores."
doc_2 = "Brassica oleracea var. italica, el br√≥coli,1‚Äã br√©col2‚Äã o br√≥quil3‚Äã del italiano broccoli (brote), es una planta de la familia de las brasic√°ceas. Existen otras variedades de la misma especie, tales como: repollo (B. o. capitata), la coliflor (B. o. botrytis), el colinabo (B. o. gongylodes) y la col de Bruselas (B. o. gemmifera). El llamado br√≥coli chino o kai-lan (B. o. alboglabra) es tambi√©n una variedad de Brassica oleracea."
doc_3 = "La bicicleta de pi√±√≥n fijo, fixie o fixed es una bicicleta monomarcha, que no tiene pi√±√≥n libre, lo que significa que no tiene punto muerto; es decir, los pedales est√°n siempre en movimiento cuando la bicicleta est√° en marcha. Esto significa que no se puede dejar de pedalear, ya que, mientras la rueda trasera gire, la cadena y los pedales girar√°n siempre solidariamente. Por este motivo, se puede frenar haciendo una fuerza inversa al sentido de la marcha, y tambi√©n ir marcha atr√°s."

documents = [doc_1, doc_2, doc_3]

from gensim.utils import simple_preprocess

def sent_to_words(sentences: list[str]) -> list[list[str]]:
    """Function convert sentences to words

    Use the tokenizer provided by gensim using
    `simple_process()` which remove punctuation and converte
    to lowercase (`deacc=True`)
    """
    return [simple_preprocess(sent, deacc=True) for sent in sentences]

docs_tokenized = sent_to_words(documents)
docs_tokenized[0][:10]

from gensim.corpora import Dictionary

gensim_dic = Dictionary()
bag_of_words_corpus = [gensim_dic.doc2bow(doc, allow_update=True) for doc in docs_tokenized]

type(gensim_dic)

for k, v in gensim_dic.iteritems():
    print(k, v)

print(len(bag_of_words_corpus))
bag_of_words_corpus[0]

def bag_to_dict(bag_of_words: list, gensim_dic: Dictionary, titles: list[str]) -> list:
    data = {}
    for doc, title in zip(bag_of_words, titles):
        data[title] = dict([(gensim_dic[id], freq) for id, freq in doc])
    return data

data = bag_to_dict(bag_of_words_corpus, gensim_dic, titles=["ADA", "BROCOLI", "FIXED"])

data

import pandas as pd

doc_matrix_simple = pd.DataFrame(data).fillna(0).astype(int).T

doc_matrix_simple

"""- Tenemos una matrix de terminos-frecuencias ($tf$). Es decir cuantas veces un termino aparece en cierto documento.
- Una variante de esta es una **BoW** binaria. ¬øC√≥mo se ver√≠a?

**¬øVen algun problema?**

- Palabras muy frecuentes que no aportan signifiancia
- Los pesos de las palabras son tratados de forma equitativa
    - Palabras muy frecuentes opacan las menos frecuentes y con mayor significado (sem√°ntico) en nuestros documentos
- Las palabras frecuentes no nos ayudarian a discriminar por ejemplo entre documentos

#### *Term frequency-Inverse Document Frequency* (TF-IDF) al rescate

<center><img src="https://media.tenor.com/Hqyg8s_gh5QAAAAd/perfectly-balanced-thanos.gif" height=250></center>

- Metodo de ponderaci√≥n creado para algoritmos de Information Retrieval
- Bueno para clasificaci√≥n de documentos y clustering
- Se calcula con la multiplicacion $tf_{d,t} \cdot idf_t$

Donde:
  - $tf_{d,t}$ es la frecuencia del termino en un documento $d$
  - $idf_t$ es la frecuencia inversa del termino en toda la colecci√≥n de documentos. Se calcula de la siguiente forma:

$$idf_t = log_2\frac{N}{df_t}$$

Entonces:

$$tf\_idf(d,t) = tf_{d,t} ‚ãÖ \log_2\frac{N}{df_t}$$

#### üßÆ Ejercicio: Aplica TF-IDF usando gensim

**HINT:** https://radimrehurek.com/gensim/models/tfidfmodel.html
"""

from gensim.models import TfidfModel

tfidf = TfidfModel(bag_of_words_corpus, smartirs="ntc")

tfidf[bag_of_words_corpus[0]]

def bag_to_dict_tfidf(bag_of_words: list, gensim_dic: Dictionary, titles: list[str]) -> list:
    data = {}
    tfidf = TfidfModel(bag_of_words, smartirs="ntc")
    for doc, title in zip(tfidf[bag_of_words], titles):
        data[title] = dict([(gensim_dic[id], freq) for id, freq in doc])
    return data

data = bag_to_dict_tfidf(bag_of_words_corpus, gensim_dic, titles=["ADA", "BROCOLI", "FIXED"])

data

doc_matrix_tfidf = pd.DataFrame(data).fillna(0).T

doc_matrix_tfidf

"""#### Calculando similitud entre vectores

<center><img src="https://cdn.acidcow.com/pics/20130320/people_who_came_face_to_face_with_their_doppelganger_19.jpg" width=500></center>

La forma estandar de obtener la similitud entre vectores para **BoW** es con la distancia coseno entre ellos

$$cos(\overrightarrow{v},\overrightarrow{w}) = \frac{\overrightarrow{v} \cdot\overrightarrow{w}}{|\overrightarrow{v}||\overrightarrow{w}|}$$

Aunque hay muchas m√°s formas de [calcular la distancia](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html) entre vectores
"""

from sklearn.metrics.pairwise import cosine_similarity

doc_1 = doc_matrix_tfidf.loc["BROCOLI"].values.reshape(1, -1)
doc_2 = doc_matrix_tfidf.loc["FIXED"].values.reshape(1, -1)
cosine_similarity(doc_1, doc_2)

"""#### Agregando m√°s documentos a nuestra bolsa

![](https://media.tenor.com/55hA4TgUrOMAAAAM/bag-bags.gif)
"""

def update_bow(doc: str, bag_of_words: list, gensim_dic: Dictionary) -> pd.DataFrame:
    words = simple_preprocess(doc, deacc=True)
    bag_of_words.append(gensim_dic.doc2bow(words, allow_update=True))
    return bag_of_words

#sample_doc = "Las bicicletas fixie, tambi√©n denominadas bicicletas de pi√±√≥n fijo, son bicis de una sola marcha, de pi√±√≥n fijo, y sin punto muerto, por lo que se debe avanzar, frenar y dar marcha atr√°s con el uso de los pedales. La rueda de atr√°s gira cuando giran los pedales. Si pedaleas hacia delante, avanzas; si paras los pedales, frenas y si pedaleas hacia atr√°s, ir√°s marcha atr√°s. Esto requiere de un entrenamiento a√±adido que la bicicleta con pi√±√≥n libre no lo necesita. No obstante, las bicicletas fixie tienen much√≠simas ventajas."
sample_doc = "El br√≥coli o br√©col es una planta de la familia de las brasic√°ceas, como otras hortalizas que conocemos como coles. Est√° por tanto emparentado con verduras como la coliflor, el repollo y las diferentes coles lisas o rizadas, incluyendo el kale o las coles de Bruselas."

new_bag = update_bow(sample_doc, bag_of_words_corpus.copy(), gensim_dic)
len(new_bag)

for k, v in gensim_dic.iteritems():
    print(k, v)

new_data = bag_to_dict_tfidf(new_bag, gensim_dic, ["ADA", "BROCOLI", "FIXED", "SAMPLE"])

new_doc_matrix_tfidf = pd.DataFrame(new_data).fillna(0).T
new_doc_matrix_tfidf

"""#### üëØ‚Äç‚ôÇÔ∏è Ejercicio: Calcula la similitud del nuevo documento con el resto de documentos"""

doc_sample_values = new_doc_matrix_tfidf.loc["SAMPLE"].values.reshape(1, -1)

doc_titles = ["ADA", "BROCOLI", "FIXED"]
for i, doc_title in enumerate(doc_titles):
    current_doc_values = new_doc_matrix_tfidf.loc[doc_title].values.reshape(1, -1)
    print(f"Similarity beetwen SAMPLE/{doc_title}= {cosine_similarity(current_doc_values, doc_sample_values)}")

"""## Pr√°ctica 2: Propiedades estad√≠sticas de la lengua

### Fecha de entrega: 2 de Marzo de 2025 11:59pm

1. Verificar si la ley de Zipf se cumple en un lenguaje artificial creado por ustedes.
    - *Ejemplo:* Un "lenguaje artificial" podr√≠a ser simplemente un texto donde las secuencias de caracteres fueron generadas aleatoriamente.

"""

def generate_word(length: int) -> str:
    """
    Generate a random word.
    Parameters:
      length: The length of the word to generate.
      seed: The seed to use for the random number generator.
    Returns:
      A string with the generated word.
    """
    vowels = "aeiou"
    consonants = "bcdfghjklmnpqrstvwxyz"
    probs_num_cons = [0.45, 0.3, 0.15, 0.1]
    probs_num_vow = [0.55, 0.3, 0.15]
    prob_begin_vowel = 0.5

    word = ""
    next_vowel = False
    if np.random.rand() < prob_begin_vowel:
        num_vow = np.random.choice([1, 2, 3], p=probs_num_vow)
        for _ in range(num_vow):
            word += random.choice(vowels)
    else:
        num_cons = np.random.choice([1, 2, 3, 4], p=probs_num_cons)
        for _ in range(num_cons):
            word += random.choice(consonants)
        next_vowel = True

    while len(word) < length:
        if next_vowel:
            num_vow = np.random.choice([1, 2, 3], p=probs_num_vow)
            for _ in range(num_vow):
                word += random.choice(vowels)
            next_vowel = False
        else:
            num_cons = np.random.choice([1, 2, 3, 4], p=probs_num_cons)
            for _ in range(num_cons):
                word += random.choice(consonants)
            next_vowel = True

    if length < len(word):
        word = word[:length]
    return word

def generate_text(length: int, matrix_transition: np.ndarray, seed: int = 42) -> str:
    """
    Generate a text with random characters.
    Parameters:
      length: The length of the text to generate.
      seed: The seed to use for the random number generator.
    Returns:
      A string with the generated text.
    """
    text = []
    length_ = np.random.poisson(3)
    if length_ < 1 or length_ > 25:
        length_ = 3
    text.append(generate_word(length_))
    while len(text) < length:
        probs_next_length = matrix_transition[length_-1]
        length_ = np.random.choice(range(1, matrix_transition.shape[0]+1), p=probs_next_length)
        text.append(generate_word(length_))

    return text

# Definir los estados: Longitudes de palabra de 1 a 25
max_length = 25
states = np.arange(1, max_length + 1)

# Crear una distribuci√≥n inicial que favorezca palabras cortas (usando una distribuci√≥n inversa)
initial_distribution = 1 / states  # M√°s peso a palabras cortas
initial_distribution /= initial_distribution.sum()  # Normalizar

# Matriz de transici√≥n: Probabilidad de ir de una longitud a otra
transition_matrix = np.zeros((max_length, max_length))

# Construir la matriz con mayor probabilidad de transiciones cercanas
for i in range(max_length):
    for j in range(max_length):
        distancia = abs(states[i] - states[j])
        transition_matrix[i, j] = np.exp(-0.5 * distancia)  # Penalizar saltos grandes

    # Favorecer la longitud actual y normalizar la fila
    transition_matrix[i] *= initial_distribution[i]
    transition_matrix[i] /= transition_matrix[i].sum()

text = pd.Series(generate_text(100000, transition_matrix))

words_counts = text.value_counts()

plt.figure(figsize=(7.5, 4.5))
plt.plot(words_counts.index, words_counts.values)
plt.xlabel('Word length')
plt.xscale('log')
plt.yscale('log')
plt.title('Word length distribution')
plt.show()

"""Podemos ver c√≥mo las frecuencias de las palabras siguen un comportamiento similar a la Ley de Zipf, esto se debe a c√≥mo se construyo el lenguaje, si bien se usan caracteres aleatorios, para la construcci√≥n de las palabras se consider√≥ la secuencia CVCV consonante - vocal / vocal - consonante para cosntruirlas considerando probabilidades altas para secuencias monovoc√°licas, un poco menos para diptongos y poco menos para triptongos; para las secuencias de consonantes se consideran tambi√©n probabilidades altas para solo una consonante y bajan gradualmente hasta secuencias de cuatro vocales.

Por otro lado, se consider√≥ tmabi√©n una matriz de transicion entre palabras modelando con una Cadena de Markov las secuencias de las longitudes de palabras, priorizando las palabras cortas y dando una probabilidad m√°s baja de transicionar a palabras largas, con esto logramos replicar un comportamiento similar a la Ley Zipf

2. Explorar `datasets` del sitio [Hugging Face](https://huggingface.co/datasets) y elegir documentos de diferentes dominios en Espa√±ol (al menos 3). Realizar reconocimiento de entidades nombradas (NER).
    - Pueden utilizar subconjuntos de los datasets encontrados
    - Mostrar resultados del reconocimiento
    - Una distribuci√≥n de frecuencias de las etiquetas m√°s comunes en cada dominio
    - Comentarios generales del desempe√±o observado.

*Sugerencias: Spacy, CoreNLP (puede ser cualquier otra herramienta)*
"""

ds1 = load_dataset("Fernandoefg/cuentos_es")
ds2 = load_dataset("dariolopez/justicio-BOE-A-1978-31229-constitucion-by-articles")
ds3 = load_dataset("Angy309/Noticias")

subset1 = [ds1['train'].data['content'][i].as_py() for i in range(5)]
subset2 = [ds2['train'].data['text'][i].as_py() for i in range(5)]
subset3 = [ds3['test'].data['text'][i].as_py() for i in range(15)]

docs1 = list(nlp.pipe(subset1))
docs2 = list(nlp.pipe(subset2))
docs3 = list(nlp.pipe(subset3))

displacy.render(docs1[0][:500], style="ent")

displacy.render(docs2, style="ent")

displacy.render(docs3, style="ent")

"""Se utilizaron tres corpus diferentes; uno de cuentos, uno de leyes del gobierno de Espa√±a y otro de noticias. Se utiliz√≥ Spacy junto con su corpus de noticias en Espa√±ol para identificar las entidades nombrada. Se pude observar que para el cuento, Spacy no hace un buen trabajo, en particular la etiqueta ORG (Organizaci√≥n) se utiliza de manera incorrecta y en general el desempe√±o es bastante malo ya que los cuentos al ser un lenguaje m√°s narrativo y descriptivo hacen poco uso de entidades nombradas.

Para el texto de las leyes de Espa√±a mejora el reconocimiento de la etiqueta PERSON esto puede deberse a que en este tipo de textos estas entidades suelen aparecer de forma muy clara para ser preciso en cuanto al lenguaje jur√≠dico, para saber en quien recae la acci√≥n y la responsabilidad.

Finalmente, en el corpus de noticias, la identificaci√≥n fue mucho m√°s precisa y en general fue donde el desempe√±o result√≥ el mejor de entre los tres; esto se debe a que es en este tipo de textos en los que Spicy fue entrenado con su corpus de noticias en Espa√±ol.
"""