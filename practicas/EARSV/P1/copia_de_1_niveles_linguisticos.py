# -*- coding: utf-8 -*-
"""Copia de 1_niveles_linguisticos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G7qJvRm8FJd_mO5zv1a0JHeevYpnoomm

# 1. Niveles Ling√º√≠sticos

## Objetivos

- Trabajar tareas a diferentes niveles ling√º√≠sticos (Fon√©tico, Morf√≥logico, Sint√°ctico)
- Manipularan y recuperar√° informaci√≥n de datasets disponibles en Github para resolver tareas de NLP
- Comparar enfoques basados en reglas y estad√≠sticos para el an√°lisis morfol√≥gico

## Fon√©tica y Fonolog√≠a

<center><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/IPA_chart_2020.svg/660px-IPA_chart_2020.svg.png"></center
"""

# Commented out IPython magic to ensure Python compatibility.
# %%HTML
# <center><iframe width='901' height='600' src='https://www.youtube.com/embed/DcNMCB-Gsn8?controls=1'></iframe></center

# Commented out IPython magic to ensure Python compatibility.
# %%HTML
# <center><iframe width='900' height='600' src='https://www.youtube.com/embed/74nnLh0Vdcc?controls=1'></iframe></center>

"""### International Phonetic Alphabet (IPA)

- Las lenguas naturales tienen muchos sonidos diferentes por lo que necesitamos una forma de describirlos independientemente de las lenguas
- IPA es una representaci√≥n escrita de los [sonidos](https://www.ipachart.com/) del [habla](http://ipa-reader.xyz/)

### Dataset: [IPA-dict](https://github.com/open-dict-data/ipa-dict) de open-dict

- Diccionario de palabras para varios idiomas con su representaci√≥n fon√©tica
- Representaci√≥n simple, una palabra por renglon con el formato:

```
[PALABRA][TAB][IPA]

Ejemplos
mariguana	/ma…æi…£wana/
zyuganov's   /Ààzju…°…ën…ëvz/, /Ààzu…°…ën…ëvz/
```

- [ISO language codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)
- URL: `https://raw.githubusercontent.com/open-dict-data/ipa-dict/master/data/<iso-lang>`

#### Explorando el corpus üó∫Ô∏è
"""

IPA_URL = "https://raw.githubusercontent.com/open-dict-data/ipa-dict/master/data/{lang}.txt"

import requests as r
from pprint import pprint as pp

response = r.get(IPA_URL.format(lang="en_US"))
response.text[:100]

ipa_data = response.text.split("\n")
#print(ipa_data[-4:])
ipa_data[-1]
pp(ipa_data[400:410])

# Puede haber mas de una transcipcion asociada a una palabra
print(ipa_data[-3].split("\t"))

for data in ipa_data[300:500]:
    word, ipa = data.split('\t')
    representations = ipa.split(", ")
    if len(representations) >= 2:
        print(f"{word} --> {representations}")

"""#### Obtenci√≥n y manipulaci√≥n"""

import http

def download_ipa_corpus(iso_lang: str) -> str:
    """Get ipa-dict file from Github

    Parameters:
    -----------
    iso_lang:
        Language as iso code

    Results:
    --------
    dict:
        Dictionary with words as keys and phonetic representation
        as values for a given lang code
    """
    print(f"Downloading {iso_lang}", end="::")
    response = r.get(IPA_URL.format(lang=iso_lang))
    status_code = response.status_code
    print(f"status={status_code}")
    if status_code != http.HTTPStatus.OK:
        print(f"ERROR on {iso_lang} :(")
        return ""
    return response.text

download_ipa_corpus("en_US").rstrip()[:50]

def parse_response(response: str) -> dict:
    """Parse text response from ipa-dict to python dict

    Each row have the format:
    [WORD][TAB]/[IPA]/(, /[IPA]/)?

    Parameters
    ----------
    response: str
        ipa-dict raw text

    Returns
    -------
    dict:
        A dictionary with the word as key and the phonetic
        representations as value
    """
    ipa_list = response.rstrip().split("\n")
    result = {}
    for item in ipa_list:
        if item == '':
            continue
        item_list = item.split("\t")
        result[item_list[0]] = item_list[1]
    return result

parse_response(download_ipa_corpus("en_US"))["ababa"]

es_mx_ipa = parse_response(download_ipa_corpus("es_MX"))

def get_ipa_transcriptions(word: str, dataset: dict) -> list[str]:
    """Search for a word in an IPA phonetics dict

    Given a word this function return the IPA transcriptions

    Parameters:
    -----------
    word: str
        A word to search in the dataset
    dataset: dict
        A dataset for a given language code

    Returns
    -------
    list[str]:
        List with posible transcriptions if any,
        else an empty list
    """
    return dataset.get(word.lower(), "").split(", ")

get_ipa_transcriptions("mayonesa", es_mx_ipa)

"""#### Obtengamos datasets"""

# Get datasets
dataset_es_mx = parse_response(download_ipa_corpus("es_MX"))
dataset_ja = parse_response(download_ipa_corpus("ja"))
dataset_en_us = parse_response(download_ipa_corpus("en_US"))
dataset_fr = parse_response(download_ipa_corpus("fr_FR"))

# Simple query
get_ipa_transcriptions("beautiful", dataset_en_us)

# Examples
print(f"dog -> {get_ipa_transcriptions('dog', dataset_en_us)} üê∂")
print(f"mariguana -> {get_ipa_transcriptions('mariguana', dataset_es_mx)} ü™¥")
print(f"Áå´ - > {get_ipa_transcriptions('Áå´', dataset_ja)} üêà")
print(f"croissant -> {get_ipa_transcriptions('croissant', dataset_fr)} ü•ê")

# Diferentes formas de pronunciar
print(f"[es_MX] hotel | {dataset_es_mx['hotel']}")
print(f"[en_US] hotel | {dataset_en_us['hotel']}")

print(f"[ja] „Éõ„ÉÜ„É´ | {dataset_ja['„Éõ„ÉÜ„É´']}")
print(f"[fr] h√¥tel | {dataset_fr['h√¥tel']}")

"""#### üßôüèº‚Äç‚ôÇÔ∏è Ejercicio: Obtener la distribuci√≥n de frecuencias de los s√≠mbolos fonol√≥gicos para espa√±ol"""

from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt

def get_phone_symbols_freq(dataset: dict):
    freqs = defaultdict(int)
    ipas = [_.strip("/") for _ in dataset.values()]
    unique_ipas = set(ipas)
    for ipa in unique_ipas:
        for char in ipa:
            freqs[char] += 1
    return freqs

freqs_es = get_phone_symbols_freq(dataset_es_mx)
# Sorted by freq number (d[1]) descendent (reverse=True)
distribution_es = dict(sorted(freqs_es.items(), key=lambda d: d[1], reverse=True))
df_es = pd.DataFrame.from_dict(distribution_es, orient='index')

"""#### Encontrar hom√≥fonos (palabras con el mismo sonido pero distina ortograf√≠a)

- Ejemplos: Casa-Caza, Vaya-Valla
"""

from collections import Counter

transcription_counts = Counter(dataset_es_mx.values())
duplicated_transcriptions = [transcription for transcription, freq in transcription_counts.items() if freq > 1]

for ipa in duplicated_transcriptions[-10:]:
    words = [word for word, transcription in dataset_es_mx.items() if transcription == ipa]
    print(f"{ipa} => {words}")

"""#### Obteniendo todos los datos"""

lang_codes = {
    "ar": "Arabic (Modern Standard)",
    "de": "German",
    "en_UK": "English (Received Pronunciation)",
    "en_US": "English (General American)",
    "eo": "Esperanto",
    "es_ES": "Spanish (Spain)",
    "es_MX": "Spanish (Mexico)",
    "fa": "Persian",
    "fi": "Finnish",
    "fr_FR": "French (France)",
    "fr_QC": "French (Qu√©bec)",
    "is": "Icelandic",
    "ja": "Japanese",
    "jam": "Jamaican Creole",
    "km": "Khmer",
    "ko": "Korean",
    "ma": "Malay (Malaysian and Indonesian)",
    "nb": "Norwegian Bokm√•l",
    "nl": "Dutch",
    "or": "Odia",
    "ro": "Romanian",
    "sv": "Swedish",
    "sw": "Swahili",
    "tts": "Isan",
    "vi_C": "Vietnamese (Central)",
    "vi_N": "Vietnamese (Northern)",
    "vi_S": "Vietnamese (Southern)",
    "yue": "Cantonese",
    "zh_hans": "Mandarin (Simplified)",
    "zh_hant": "Mandarin (Traditional)"
}
iso_lang_codes = list(lang_codes.keys())

def get_corpora() -> dict:
    """Download corpora from ipa-dict github

    Given a list of iso lang codes download available datasets.

    Returns
    -------
    dict
        Lang codes as keys and dictionary with words-transcriptions
        as values
    """
    return {
        code: parse_response(download_ipa_corpus(code))
         for code in iso_lang_codes
        }

data = get_corpora()

"""#### Sistema de b√∫squeda (na√Øve)"""

from rich import print as rprint
from rich.columns import Columns
from rich.panel import Panel
from rich.text import Text

def get_formated_string(code: str, name: str):
    return f"[b]{name}[/b]\n[yellow]{code}"

rprint(Panel(Text("Representaci√≥n fon√©tica de palabras", style="bold", justify="center")))
rendable_langs = [Panel(get_formated_string(code, lang), expand=True) for code, lang in lang_codes.items()]
rprint(Columns(rendable_langs))

lang = input("lang>> ")
rprint(f"Selected language: {lang_codes[lang]}") if lang else rprint("Adios üëãüèº")
while lang:
    sub_dataset = data[lang]
    query = input(f"  [{lang}]word>> ")
    results = get_ipa_transcriptions(query, sub_dataset)
    rprint(query, " | ", ", ".join(results))
    while query:
        query = input(f"  [{lang}]word>> ")
        if query:
            results = get_ipa_transcriptions(query, sub_dataset)
            rprint(query, " | ", ", ".join(results))
    lang = input("lang>> ")
    rprint(f"Selected language: {lang_codes[lang]}") if lang else rprint("Adios üëãüèº")

"""#### üë©‚Äçüî¨ Ejercicio: Obtener palabras con pronunciaci√≥n similar"""

from collections import defaultdict

def get_rhyming_patterns(sentence: str, dataset: dict) -> dict[str, list]:
    words = sentence.split()
    word_ipa_map = {}
    for word in words:
        ipa_transcriptions = get_ipa_transcriptions(word, dataset)
        # Remove "/" char from transcriptions
        word_ipa_map.update({word: [_.strip("/") for _ in ipa_transcriptions]})

    rhyming_patterns = defaultdict(list)
    for word, ipas in word_ipa_map.items():
        for ipa in ipas:
            # Getting last 2 elements of the ipa representation
            pattern = ipa[-2:]
            rhyming_patterns[pattern].append(word)
    return rhyming_patterns

def display_rhyming_patterns(patterns: dict[str, list]) -> None:
    for pattern, words in patterns.items():
        if len(set(words)) > 1:
            print(f"{pattern}:: {', '.join(words)}")

"""#### Testing

```
…£o:: juego, fuego
on:: con, coraz√≥n
 éa:: brilla, orilla
```
"""

#sentence = "There once was a cat that ate a rat and after that sat on a yellow mat"
#sentence = "the cat sat on the mat and looked at the rat."
sentence = "If you drop the ball it will fall on the doll"
#sentence = "cuando juego con fuego siento como brilla la orilla de mi coraz√≥n"

dataset = data.get("en_US")
rhyming_words = get_rhyming_patterns(sentence, dataset)
display_rhyming_patterns(rhyming_words)

"""#### Material extra (fon√©tica)"""

!apt-get install espeak -y

!espeak -v es "Hola que hace" -w mi-prueba.wav

"""## Morfolog√≠a

<center><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/29/Flexi%C3%B3nGato-svg.svg/800px-Flexi%C3%B3nGato-svg.svg.png" height=300></center>

> De <a href="//commons.wikimedia.org/wiki/User:KES47" class="mw-redirect" title="User:KES47">KES47</a> - <a href="//commons.wikimedia.org/wiki/File:Flexi%C3%B3nGato.png" title="File:Flexi√≥nGato.png">File:Flexi√≥nGato.png</a> y <a href="//commons.wikimedia.org/wiki/File:Nuvola_apps_package_toys_svg.svg" title="File:Nuvola apps package toys svg.svg">File:Nuvola apps package toys svg.svg</a>, <a href="http://www.gnu.org/licenses/lgpl.html" title="GNU Lesser General Public License">LGPL</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=27305101">Enlace</a>

El an√°lisis morfol√≥gico es la determinaci√≥n de las partes que componen la palabra y su representaci√≥n ling√º√≠stica, es una especie de etiquetado

Los elementos morfol√≥gicos son analizados para:

- Determinar la funci√≥n morfol√≥gica de las palabras
- Hacer filtrado y pre-procesamiento de text

### An√°lisis morfol√≥gico basado en reglas

Recordemos que podemos hacer un analizador morfol√≥gico haciendo uso de un transductor que vaya leyendo y haciendo transformaciones en una cadena. Formalmente:

* $Q = \{q_0, \ldots, q_T\}$ conjunto finito de estados.
* $\Sigma$ es un alfabeto de entrada.
* $q_0 \in Q$ es el estado inicial.
* $F \subseteq Q$ es el conjunto de estados finales.

Un transductor es una 6-tupla $T = (Q, \Sigma, \Delta, q_0, F, \sigma)$ tal que

* $\Delta$ es un alfabeto de salida teminal
* $\Sigma$ es un alfabeto de entrada no terminal
* $\sigma: Q \times \Sigma \times \Delta \longrightarrow Q$ funci√≥n de transducci√≥n

#### EJEMPLO: Parsing con expresiones regulares

Con fines de pr√°cticidad vamos a _imitar_ el comportamiento de un transductor utilizando el modulo de python `re`

La estructura del sustantivo en espa√±ol es:

` BASE+AFIJOS (marcas flexivas)   --> Base+DIM+GEN+NUM`
"""

palabras = [
    'ni√±o',
    'ni√±os',
    'ni√±as',
    'ni√±itos',
    'gato',
    'gatos',
    'gatitos',
    'perritos',
    'paloma',
    'palomita',
    'palomas',
    'flores',
    'flor',
    'florecita',
    'l√°piz',
    'l√°pices',
    # 'chiquitititititos',
    #'curriculum', # curricula
    #'campus', # campi
]

import re

def morph_parser_rules(words: list[str]) -> list[str]:
    """Aplica reglas morfol√≥gicas a una lista de palabras para realizar
    un an√°lisis morfol√≥gico.

    Parameters:
    ----------
    words : list of str
        Lista de palabras a las que se les aplicar√°n las reglas morfol√≥gicas.

    Returns:
    -------
    list of str
        Una lista de palabras despu√©s de aplicar las reglas morfol√≥gicas.
    """

    # Lista para guardar las palabras parseadas
    morph_parsing = []

    # Reglas que capturan ciertos morfemas
    # {ecita, itos, as, os}
    for w in words:
        # ecit -> DIM
        R0 = re.sub(r'([^ ]+)ecit([a|o|as|os])', r'\1-DIM\2', w)
        # it -> DIM
        R1 = re.sub(r'([^ ]+)it([a|o|as|os])', r'\1-DIM\2', R0)
        # a(s) -> FEM
        R2 = re.sub(r'([^ ]+)a(s)', r'\1-FEM\2', R1)
        # a -> FEM
        R3 = re.sub(r'([^ ]+)a\b', r'\1-FEM', R2)
        # o(s) -> MSC
        R4 = re.sub(r'([^ ]+)o(s)', r'\1-MSC\2', R3)
        # o .> MSC
        R5 = re.sub(r'([^ ]+)o\b', r'\1-MSC', R4)
        # es -> PL
        R6 = re.sub(r'([^ ]+)es\b', r'\1-PL', R5)
        # s -> PL
        R7 = re.sub(r'([^ ]+)s\b', r'\1-PL', R6)
        # Sustituye la c por z cuando es necesario
        parse = re.sub(r'c-', r'z-', R7)

        # Guarda los parseos
        morph_parsing.append(parse)
    return morph_parsing

morph_parsing = morph_parser_rules(palabras)
for palabra, parseo in zip(palabras, morph_parsing):
    print(palabra, "-->", parseo)

"""#### Preguntas ü§î
- ¬øQu√© pasa con las reglas en lenguas donde son m√°s comunes los prefijos y no los sufijos?
- ¬øC√≥mo podr√≠amos identificar caracter√≠sticas de las lenguas?

#### Herramientas para hacer sistemas de an√°lisis morfol√≥gico basados en reglas

- [Apertium](https://en.wikipedia.org/wiki/Apertium)
- [Foma](https://github.com/mhulden/foma/tree/master)
- [Helsinki Finite-State Technology](https://hfst.github.io/)
- Ejemplo [proyecto](https://github.com/apertium/apertium-yua) de analizador morfol√≥gico de Maya Yucateco
- Ejemplo normalizador ortogr√°fico del [N√°huatl](https://github.com/ElotlMX/py-elotl/tree/master)


Tambi√©n se pueden utilizar diferentes m√©todos de aprendizaje de m√°quina para realizar an√°lisis/generaci√≥n morfol√≥gica. En los √∫ltimos a√±os ha habido un shared task de [morphological reinflection](https://github.com/sigmorphon/2023InflectionST) para poner a competir diferentes m√©todos

### Segmentaci√≥n morfol√≥gica

#### Corpus: [SIGMORPHON 2022 Shared Task on Morpheme Segmentation](https://github.com/sigmorphon/2022SegmentationST/tree/main)

- Shared task donde se buscaba convertir las palabras en una secuencia de morfemas
- Dividido en dos partes:
    - Segmentaci√≥n a nivel de palabras (nos enfocaremos en esta)
    - Segmentaci√≥n a nivel oraciones

#### Track: words

| word class | Description                      | English example (input ==> output)     |
|------------|----------------------------------|----------------------------------------|
| 100        | Inflection only                  | played ==> play @@ed                   |
| 010        | Derivation only                  | player ==> play @@er                   |
| 101        | Inflection and Compound          | wheelbands ==> wheel @@band @@s        |
| 000        | Root words                       | progress ==> progress                  |
| 011        | Derivation and Compound          | tankbuster ==> tank @@bust @@er        |
| 110        | Inflection and Derivation        | urbanizes ==> urban @@ize @@s          |
| 001        | Compound only                    | hotpot ==> hot @@pot                   |
| 111        | Inflection, Derivation, Compound | trackworkers ==> track @@work @@er @@s

#### Explorando el corpus
"""

response = r.get("https://raw.githubusercontent.com/sigmorphon/2022SegmentationST/main/data/spa.word.test.gold.tsv")
response.text[:100]

raw_data = response.text.split("\n")
raw_data[-2]

element = raw_data[2].split("\t")
element

element[1].split()

LANGS = {
    "ces": "Czech",
    "eng": "English",
    "fra": "French",
    "hun": "Hungarian",
    "spa": "Spanish",
    "ita": "Italian",
    "lat": "Latin",
    "rus": "Russian",
}
CATEGORIES = {
    "100": "Inflection",
    "010": "Derivation",
    "101": "Inflection, Compound",
    "000": "Root",
    "011": "Derivation, Compound",
    "110": "Inflection, Derivation",
    "001": "Compound",
    "111": "Inflection, Derivation, Compound"
}

def get_track_files(lang: str, track: str = "word") -> list[str]:
    """Genera una lista de nombres de archivo del shared task

    Con base en el idioma y el track obtiene el nombre de los archivos
    para con informaci√≥n reelevante para hacer an√°lisis estad√≠stico.
    Esto es archivos .test y .dev

    Parameters:
    ----------
    lang : str
        Idioma para el cual se generar√°n los nombres de archivo.
    track : str, optional
        Track del shared task de donde vienen los datos (por defecto es "word").

    Returns:
    -------
    list[str]
        Una lista de nombres de archivo generados para el idioma y la pista especificados.
    """
    return [
        f"{lang}.{track}.test.gold",
        f"{lang}.{track}.dev",
    ]

def get_raw_corpus(files: list) -> list:
    """Descarga y concatena los datos de los archivos tsv desde una URL base.

    Parameters:
    ----------
    files : list
        Lista de nombres de archivos (sin extensi√≥n) que se descargar√°n
        y concatenar√°n.

    Returns:
    -------
    list
        Una lista que contiene los contenidos descargados y concatenados
        de los archivos tsv.
    """
    result = []
    for file in files:
        print(f"Downloading {file}.tsv", end=" ")
        response = r.get(f"https://raw.githubusercontent.com/sigmorphon/2022SegmentationST/main/data/{file}.tsv")
        print(f"status={response.status_code}")
        lines = response.text.split("\n")
        result.extend(lines[:-1])
    return result

import pandas as pd

def raw_corpus_to_dataframe(corpus_list: list, lang: str) -> pd.DataFrame:
    """Convierte una lista de datos de corpus en un DataFrame

    Parameters:
    ----------
    corpus_list : list
        Lista de l√≠neas del corpus a convertir en DataFrame.
    lang : str
        Idioma al que pertenecen los datos del corpus.

    Returns:
    -------
    pd.DataFrame
        Un DataFrame de pandas que contiene los datos del corpus procesados.
    """
    data_list = []
    for line in corpus_list:
        try:
            word, tagged_data, category = line.split("\t")
        except ValueError:
            # Caso donde no existe la categoria
            word, tagged_data = line.split("\t")
            category = "NOT_FOUND"
        morphemes = tagged_data.split()
        data_list.append({"words": word, "morph": morphemes, "category": category, "lang": lang})
    df = pd.DataFrame(data_list)
    df["word_len"] = df["words"].apply(lambda x: len(x))
    df["morph_count"] = df["morph"].apply(lambda x: len(x))
    return df

files = get_track_files("spa")
raw_spa = get_raw_corpus(files)
df = raw_corpus_to_dataframe(raw_spa, lang="spa")

df.head()

"""#### An√°lisis cuantitativo para el Espa√±ol"""

print("Total unique words:", len(df["words"].unique()))
df["category"].value_counts().head(30)

df["word_len"].mean()

from matplotlib import pyplot as plt

plt.hist(df['word_len'], bins=10, edgecolor='black')
plt.xlabel('Word Length')
plt.ylabel('Frequency')
plt.title('Word Length Distribution')
plt.show()

def plot_histogram(df, kind, lang):
    """Genera un histograma de frecuencia para una columna espec√≠fica
    en un DataFrame.

    Parameters:
    ----------
    df : pd.DataFrame
        DataFrame que contiene los datos para generar el histograma.
    kind : str
        Nombre de la columna para la cual se generar√° el histograma.
    lang : str
        Idioma asociado a los datos.

    Returns:
    -------
    None
        Esta funci√≥n muestra el histograma usando matplotlib.
    """
    counts = df[kind].value_counts().head(30)
    plt.bar(counts.index, counts.values)
    plt.xlabel(kind)
    plt.ylabel('Frequency')
    plt.title(f'{kind} Frequency Graph for {lang}')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

plot_histogram(df, "category", "spa")

"""#### Morfosintaxis

- Etiquetas que hacen expl√≠cita la funcion gramatical de las palabras en una oraci√≥n
- Determina la funci√≥n de la palabra dentro la oraci√≥n (por ello se le llama Partes del Discurso)
- Se le conoce tambien como **An√°lisis morfosint√°ctico**: es el puente entre la estructura de las palabras y la sintaxis
- Permiten el desarrollo de herramientas de NLP m√°s avanzadas
- El etiquetado es una tarea que se puede abordar con t√©cnicas secuenciales, por ejemplo, HMMs, CRFs, Redes neuronales

<center><img src="https://byteiota.com/wp-content/uploads/2021/01/POS-Tagging.jpg" height=500 width=500></center

#### Ejemplo

> El gato negro rie malvadamente

- El - DET
- gato - NOUN
- negro - ADJ
- r√≠e - VER

<center><img src="https://i.pinimg.com/originals/0e/f1/30/0ef130b255ea704625b2ad473701dee5.gif"></center

### Etiquetado POS usando Conditional Random Fields (CRFs)

- Modelo de gr√°ficas **no dirigido**. Generaliza los *HMM*
    - Adi√≥s a la *Markov assuption*
    - Podemos tener cualquier dependencia que queramos entre nodos
    - Nos enfocaremos en un tipo en concreto: *LinearChain-CRFs* ¬°¬øPor?!

<center><img width=300 src="https://i.kym-cdn.com/entries/icons/original/000/032/676/Unlimited_Power_Banner.jpg"></center>

- Modela la probabilidad **condicional** $P(Y|X)$
    - Modelo discriminativo
    - Probabilidad de un estado oculto dada **toda** la secuecia de entrada
![homer](https://media.tenor.com/ul0qAKNUm2kAAAAd/hiding-meme.gif)

- Captura mayor **n√∫mero de dependencias** entre las palabras y captura m√°s caracter√≠sticas
    - Estas se definen en las *feature functions* üôÄ
- El entrenamiento se realiza aplicando gradiente decendente y optimizaci√≥n con algoritmos como [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)


<center><img src="https://iameo.github.io/images/gradient-descent-400.gif"></center>

$P(\overrightarrow{y}|\overrightarrow{x}) = \frac{1}{Z} \displaystyle\prod_{i=1}^N exp\{w^T ‚ãÖ \phi(y_{i-1}, y_i, \overrightarrow{x}, i)\}$

Donde:
- $\overrightarrow{y}$ = Etiquetas POS
- $\overrightarrow{x}$ = Palabras en una oraci√≥n
- $w^T$ = Vector de pesos a aprender
- $\phi$ = Vector de *Features*
    - Calculado con base en un conjunto de *feature functions*
- $i$ = la posici√≥n actual en la oraci√≥n
- $Z$ = factor de normalizaci√≥n

![](https://aman.ai/primers/ai/assets/conditional-random-fields/Conditional_Random_Fields.png)

Tomado de http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/

#### Feature functions

$\phi(y_{i-1}, y_i, \overrightarrow{x}, i)$

- Parte fundamental de los CRFs
- Cuatro argumentos:
    - Todos los datos observables $\overrightarrow{x}$ (conectar $x$ con cualquier $y$)
    - El estado oculto anterior $y_{i-1}$
    - El estado oculto actual $y_i$
    - El index del timestamp $i$
        - Cada feature list puede tener diferentes formas

- Aqui es donde esta la flexibilidad del modelo
- Tantas features como querramos, las que consideremos que pueden ayudar a que el modelo tenga un mejor desempe√±o
    - Intimamente ligadas a la lengua. Para mejor desempe√±o se debe hacer un an√°lisis de sus caracter√≠sticas.
- Ejemplo:

```python
[
    "word.lower()",
    "EOS",
    "BOS",
    "postag",
    "pre-word",
    "nxt-word",
    "word-position",
    ...
]
```

### Implementaci√≥n de CRFs
"""

!pip install nltk
!pip install scikit-learn
!pip install -U sklearn-crfsuite

"""#### Obteniendo otro corpus m√°s"""

import nltk

# Descargando el corpus cess_esp: https://www.nltk.org/book/ch02.html#tab-corpora
nltk.download('cess_esp')

from nltk.corpus import cess_esp
# Cargando oraciones
corpora = cess_esp.tagged_sents()

corpora[1]

import requests

def get_tags_map() -> dict:
    tags_raw = requests.get("https://gist.githubusercontent.com/vitojph/39c52c709a9aff2d1d24588aba7f8155/raw/af2d83bc4c2a7e2e6dbb01bd0a10a23a3a21a551/universal_tagset-ES.map").text.split("\n")
    tags_map = {line.split("\t")[0].lower(): line.split("\t")[1] for line in tags_raw}
    return tags_map

def map_tag(tag: str, tags_map=get_tags_map()) -> str:
    return tags_map.get(tag.lower(), "N/F")

def parse_tags(corpora: list[list[tuple]]) -> list[list[tuple]]:
    result = []
    for sentence in corpora:
        print
        result.append([(word, map_tag(tag)) for word, tag in sentence if tag not in ["Fp", "Fc", "Fpa", "Fpt"]])
    return result

corpora = parse_tags(corpora)

corpora[0]

"""#### Feature lists"""

def word_to_features(sent, i):
    word = sent[i][0]
    features = {
        'word.lower()': word.lower(),
        'word[-3:]': word[-3:],
        'word[-2:]': word[-2:],
        'word.isupper()': word.isupper(),
        'word.istitle()': word.istitle(),
        'word.isdigit()': word.isdigit(),
        'prefix_1': word[:1],
        'prefix_2': word[:2],
        'suffix_1': word[-1:],
        'suffix_2': word[-2:],
        'word_len': len(word)
    }
    if i > 0:
        prev_word = sent[i - 1][0]
        features.update({
            'prev_word.lower()': prev_word.lower(),
            'prev_word.istitle()': prev_word.istitle(),
        })
    else:
        features['BOS'] = True  # Beginning of sentence

    return features

# Extract features and labels
def sent_to_features(sent) -> list:
    return [word_to_features(sent, i) for i in range(len(sent))]

def sent_to_labels(sent) -> list:
    return [label for token, label in sent]

# ¬øCuantas oraciones tenemos disponibles?
len(corpora)

# Preparando datos para el CRF
X = [[word_to_features(sent, i) for i in range(len(sent))] for sent in corpora]
y = [[pos for _, pos in sent] for sent in corpora]

# Exploraci√≥n de data estructurada
X[0]

from sklearn.model_selection import train_test_split
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

assert len(X_train) + len(X_test) == len(corpora), "Something wrong with my split :("
assert len(y_train) + len(y_test) == len(corpora), "Something wrong with my split :("

from inspect import Attribute
from sklearn_crfsuite import CRF
# Initialize and train the CRF tagger: https://sklearn-crfsuite.readthedocs.io/en/latest/api.html
crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True, verbose=True)
try:
    crf.fit(X_train, y_train)
except AttributeError as e:
    print(e)

from sklearn.metrics import classification_report
y_pred = crf.predict(X_test)

# Flatten the true and predicted labels
y_test_flat = [label for sent_labels in y_test for label in sent_labels]
y_pred_flat = [label for sent_labels in y_pred for label in sent_labels]

# Evaluate the model
report = classification_report(y_true=y_test_flat, y_pred=y_pred_flat)
print(report)

"""## Tarea 1: Niveles del lenguaje

### FECHA DE ENTREGA: 16 de Febrero 2025 at 11:59pm

### Fon√©tica

1. Si tenemos un sistema de b√∫squeda que recibe una palabra ortogr√°fica y devuelve sus transcripciones fonol√≥gicas, proponga una soluci√≥n para los casos en que la palabra buscada no se encuentra en el lexic√≥n/diccionario. *¬øC√≥mo devolver o aproximar su transcripci√≥n fonol√≥gica?*
  - Reutiliza el sistema de b√∫squeda visto en clase y mejoralo con esta funcionalidad

### Morfolog√≠a

2. Obtenga los datos de `test` y `dev` para todas las lenguas disponibles en el Shared Task SIGMORPHON 2022 y haga lo siguiente:
    - En un plot de 4 columnas y 2 rows muestre las siguientes distribuciones (un subplot por lengua):
        - Plot 1: distribuci√≥n de longitud de palabras
        - Plot 2: distribuci√≥n de la cuenta de morfemas
        - Plot 3: distribuci√≥n de categorias (si existe para la lengua)
    - Realice una funci√≥n que imprima por cada lengua lo siguiente:
        - Total de palabras
        - La longitud de palabra promedio
        - La cuenta de morfemas promedio
        - La categor√≠a m√°s com√∫n
    - Con base en esta informaci√≥n elabore una conclusi√≥n ling√º√≠stica sobre la morfolog√≠a de las lenguas analizadas.
    
### EXTRA:

- Imprimir la [matr√≠z de confusi√≥n](https://en.wikipedia.org/wiki/Confusion_matrix) para el etiquetador CRFs visto en clase y elaborar una conclusi√≥n sobre los resultados

#Tarea Fon√©tica
"""

!pip install python-Levenshtein

var='hola'
var_set=set(var)
print(var_set)

import Levenshtein
def get_similar_words(word,words_to_compare)->dict:
  word=word.lower()
  levenshtein_distances={}
  for word_compare in words_to_compare:
    word_compare=word_compare.lower()
    levenshtein_distance=Levenshtein.distance(word,word_compare)
    levenshtein_distances[word_compare]=levenshtein_distance
  #Ordenados por similitud
  levenshtein_distances=dict(sorted(levenshtein_distances.items(),key=lambda x:x[1],reverse=False))
  #10 palabras m√°s similares
  levenshtein_distances=dict(list(levenshtein_distances.items())[:10])
  return levenshtein_distances

print(get_similar_words('perrro',data['es_MX'].keys()))

rprint(Panel(Text("Representaci√≥n fon√©tica de palabras", style="bold", justify="center")))
rendable_langs = [Panel(get_formated_string(code, lang), expand=True) for code, lang in lang_codes.items()]
rprint(Columns(rendable_langs))

lang = input("lang>> ")
rprint(f"Selected language: {lang_codes[lang]}") if lang else rprint("Adios üëãüèº")
while lang:
    sub_dataset = data[lang]
    query = input(f"  [{lang}]word>> ")
    results = get_ipa_transcriptions(query, sub_dataset)
    if results == [""]:
      rprint(f'Tu palabra no est√° en el dataset. \nPero, te damos 10 posibles sugerencias similares.')
      similar_words=get_similar_words(query,sub_dataset.keys())
      for similar_word in similar_words.keys():
        results = get_ipa_transcriptions(similar_word, sub_dataset)
        rprint(similar_word, " | ", ", ".join(results))
    else:
     rprint(query, " | ", ", ".join(results))
    while query:
        query = input(f"  [{lang}]word>> ")
        if query:
            results = get_ipa_transcriptions(query, sub_dataset)
            if results == [""]:
              rprint(f'Tu palabra no est√° en el dataset.\nPero, te damos 10 posibles sugerencias similares.')
              similar_words=get_similar_words(query,sub_dataset.keys())
              for similar_word in similar_words.keys():
                      results = get_ipa_transcriptions(similar_word, sub_dataset)
                      rprint(similar_word, " | ", ", ".join(results))
            else:
             rprint(query, " | ", ", ".join(results))
    lang = input("lang>> ")
    rprint(f"Selected language: {lang_codes[lang]}") if lang else rprint("Adios üëãüèº")

"""#Tarea Morfolog√≠a"""

import requests as r
from pprint import pprint as pp
import pandas as pd
import matplotlib.pyplot as plt

def get_track_files(lang: str, track: str = "word") -> list[str]:
    """Genera una lista de nombres de archivo del shared task

    Con base en el idioma y el track obtiene el nombre de los archivos
    para con informaci√≥n reelevante para hacer an√°lisis estad√≠stico.
    Esto es archivos .test y .dev

    Parameters:
    ----------
    lang : str
        Idioma para el cual se generar√°n los nombres de archivo.
    track : str, optional
        Track del shared task de donde vienen los datos (por defecto es "word").

    Returns:
    -------
    list[str]
        Una lista de nombres de archivo generados para el idioma y la pista especificados.
    """
    return [
        f"{lang}.{track}.test.gold",
        f"{lang}.{track}.dev",
    ]

def get_raw_corpus(files: list) -> list:
    """Descarga y concatena los datos de los archivos tsv desde una URL base.

    Parameters:
    ----------
    files : list
        Lista de nombres de archivos (sin extensi√≥n) que se descargar√°n
        y concatenar√°n.

    Returns:
    -------
    list
        Una lista que contiene los contenidos descargados y concatenados
        de los archivos tsv.
    """
    result = []
    for file in files:
        #print(f"Downloading {file}.tsv", end=" ")
        response = r.get(f"https://raw.githubusercontent.com/sigmorphon/2022SegmentationST/main/data/{file}.tsv")
        #print(f"status={response.status_code}")
        lines = response.text.split("\n")
        result.extend(lines[:-1])
    return result

def raw_corpus_to_dataframe(corpus_list: list, lang: str) -> pd.DataFrame:
    """Convierte una lista de datos de corpus en un DataFrame

    Parameters:
    ----------
    corpus_list : list
        Lista de l√≠neas del corpus a convertir en DataFrame.
    lang : str
        Idioma al que pertenecen los datos del corpus.

    Returns:
    -------
    pd.DataFrame
        Un DataFrame de pandas que contiene los datos del corpus procesados.
    """
    data_list = []
    for line in corpus_list:
        try:
            word, tagged_data, category = line.split("\t")
        except ValueError:
            # Caso donde no existe la categoria
            word, tagged_data = line.split("\t")
            category = "NOT_FOUND"
        morphemes = tagged_data.split()
        data_list.append({"words": word, "morph": morphemes, "category": category, "lang": lang})
    df = pd.DataFrame(data_list)
    df["word_len"] = df["words"].apply(lambda x: len(x))
    df["morph_count"] = df["morph"].apply(lambda x: len(x))
    return df

def plot_histogram(df, kind, lang):
    """Genera un histograma de frecuencia para una columna espec√≠fica
    en un DataFrame.

    Parameters:
    ----------
    df : pd.DataFrame
        DataFrame que contiene los datos para generar el histograma.
    kind : str
        Nombre de la columna para la cual se generar√° el histograma.
    lang : str
        Idioma asociado a los datos.

    Returns:
    -------
    None
        Esta funci√≥n muestra el histograma usando matplotlib.
    """
    counts = df[kind].value_counts().head(30)
    plt.bar(counts.index, counts.values)
    plt.xlabel(kind)
    plt.ylabel('Frequency')
    plt.title(f'{kind} Frequency Graph for {lang}')
    plt.xticks(rotation=90)
    plt.tight_layout()
    #plt.show()

LANGS = {
    "ces": "Czech",
    "eng": "English",
    "fra": "French",
    "hun": "Hungarian",
    "spa": "Spanish",
    "ita": "Italian",
    "lat": "Latin",
    "rus": "Russian",
}
CATEGORIES = {
    "100": "Inflection",
    "010": "Derivation",
    "101": "Inflection, Compound",
    "000": "Root",
    "011": "Derivation, Compound",
    "110": "Inflection, Derivation",
    "001": "Compound",
    "111": "Inflection, Derivation, Compound"
}

import numpy as np
x = np.linspace(0, 10, 100)
y = np.sin(x)

columns=["word_len","morph_count","category"]
for column in columns:
  plt.figure(figsize=(20, 10))
  plt.suptitle(f"Distribuci√≥n de la columna: {column}")

  i=1
  for lang_short, lang_name in LANGS.items():
      files = get_track_files(lang_short)
      raw_corpus = get_raw_corpus(files)
      df = raw_corpus_to_dataframe(raw_corpus, lang=lang_short)
      plt.subplot(2, 4, i)
      plot_histogram(df, column, lang_name)

      i+=1
  plt.show()
  print("\n")

"""Realice una funci√≥n que imprima por cada lengua lo siguiente:
Total de palabras
La longitud de palabra promedio
La cuenta de morfemas promedio
La categor√≠a m√°s com√∫n
"""

def stats(df):
  print(f"Total de palabras: {df['words'].count()}")
  print(f"Longitud de palabra promedio: {df['word_len'].mean()}")
  print(f"Cuenta de morfemas promedio: {df['morph_count'].mean()}")
  print(f"Categor√≠a m√°s com√∫n: {df['category'].value_counts().idxmax()}")

for lang_short, lang_name in LANGS.items():
  files = get_track_files(lang_short)
  raw_corpus = get_raw_corpus(files)
  df = raw_corpus_to_dataframe(raw_corpus, lang=lang_short)
  print(f"Language: {lang_name}")
  stats(df)
  print("\n")

"""#Conclusi√≥n
Con base a los histogramas y a los promedios podemos observar varias cosas muy curiosas. Para empezar, la distribuci√≥n de longitud de palabras parecen muy similares a pesar de ser diferentes lenguas, adem√°s todas tienen una media de longitud de palabas de alrededor de 10 caracteres. Adem√°s, pasa algo muy similar con la cantidad de morfemas, la distribuci√≥n es muy parecida y el promedio est√° muy cercano a 3. En la parte de las categor√≠as s√≠ parece haber un poco m√°s de diferencas, sin embargo en la mayor√≠a predomina la categor√≠a 100 y 110, lo que corresponde "inflexi√≥n" y a "inflexi√≥n, derivaci√≥n", respectivamente. Esto a pesar de que la cantidad de palabras es muy diferente entre s√≠, lo que nos habla de que pareciera haber alguna raz√≥n por la cual todas las lenguas se relacionan entre s√≠, esto puede deberse al principio de m√≠nimo esfuerzo postulado por Zipf, donde los sistemas ling√ºisticos tienden a optimizar recursos, adem√°s, con respecto a las categor√≠as la inflexi√≥n es muy importante para cumplir funciones sint√°cticas y contextualizar palabras dentro de oraciones, por lo que es com√∫n que sea la categor√≠a m√°s frecuente sin importar la lengua. Esto nos habla de que no importa la variaci√≥n cultural, hay principios m√°s b√°sicos y arraigados en la naturaleza humana.

#Extra
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
labels = sorted(set(y_test_flat))  # Asegura que las etiquetas est√°n ordenadas

cm=confusion_matrix(y_test_flat,y_pred_flat,labels=labels)
plt.figure(figsize=(15,10))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.show()