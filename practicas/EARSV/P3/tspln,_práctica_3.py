# -*- coding: utf-8 -*-
"""TSPLN, Pr치ctica 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vdxTVEkddWJSK5piiVlTfXTIrFNtolE8
"""

!pip install spacy datasets
!python -m spacy download es_core_news_md

#En este punto se requiere reiniciar la sesi칩n para que no d칠 error.
!pip install gensim

from collections import Counter, defaultdict
from datasets import load_dataset
import matplotlib.pyplot as plt
from itertools import combinations
import numpy as np
import spacy
from spacy.tokens import Doc

from sklearn.decomposition import PCA, TruncatedSVD
import random
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

"""# 3. Pr치ctica: Vectores a palabras

**Fecha de entrega: 16 de Marzo de 2025 @ 11:59pm**

Obtenga la matriz de co-ocurrencia para un corpus en espa침ol y realice los siguientes calculos:
- Las probabilidades conjuntas
$$p(w_i,w_j) = \frac{c_{i,j}}{\sum_i \sum_j c_{i,j}}$$
- Las probabilidades marginales
$$p(w_i) = \sum_j p(w_i,w_j)$$
- Positive Point Wise Mutual Information (PPMI):
$$PPMI(w_i,w_j) = \max\{0, \log_2 \frac{p(w_i,w_j)}{p(w_i)p(w_j)}\}$$

**Comparaci칩n de representaciones**

Aplica reducci칩n de dimensionalidad (a 2D) de los vectores de la matr칤z con PPMI y de los vectores entrenados en espa침ol:

- Realiza un plot de 100 vectores aleatorios (que esten tanto en la matr칤z como en los vectores entrenados)
- Compara los resultados de los plots:
    - 쯈u칠 representaci칩n dir칤as que captura mejor relaciones sem치nticas?
    - Realiza un cuadro comparativo de ambos m칠todos con ventajas/desventajas

### 游늬 [Carpeta con vectores](https://drive.google.com/drive/folders/1reor2FGsfOB6m3AvfCE16NOHltAFjuvz?usp=drive_link)

#Matrix de Co-ocurrencia.
"""

ds_oscar = load_dataset("djstrong/oscar-small", "unshuffled_deduplicated_es")
ds_oscar=ds_oscar["train"]

nlp = spacy.load("es_core_news_md")

# Solo tokenizaci칩n y lematizaci칩n
def preprocesar(texto: str) -> list:
    doc = nlp(texto)
    tokens=[]
    for token in doc:
        if not token.is_stop and token.is_alpha and len(token.lemma_) > 2:
            tokens.append(token.lemma_.lower())
    tokens = [
        token.lemma_.lower()
        for token in doc
        if not token.is_stop
        and token.is_alpha
        and len(token.lemma_) > 2
    ]
    return tokens

docs_procesados = [preprocesar(texto) for texto in ds_oscar[:5000]["text"]]  # 5K documentos para prueba

print(docs_procesados[0])

# Contar frecuencias
frecuencias = defaultdict(int)
for doc in docs_procesados:
    for palabra in doc:
        frecuencias[palabra] += 1

# Vocabulario reducido (top 3000 palabras)
vocabulario = sorted(frecuencias, key=frecuencias.get, reverse=True)[:3000]
word2idx = {word: idx for idx, word in enumerate(vocabulario)}
vocab_size = len(vocabulario)

# Inicializar matriz
window_size = 5
co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)

# Llenar matriz
for doc in docs_procesados:
    for i, palabra in enumerate(doc):
        if palabra not in word2idx:
            continue
        idx_palabra = word2idx[palabra]
        # Ventana de contexto
        start = max(0, i - window_size)
        end = min(len(doc), i + window_size + 1)
        for j in range(start, end):
            if j == i:
                continue
            contexto = doc[j]
            if contexto in word2idx:
                idx_contexto = word2idx[contexto]
                co_matrix[idx_palabra, idx_contexto] += 1

# Probabilidad conjunta
total = co_matrix.sum().astype(float)
p_conjunta = co_matrix / total

# Marginales
p_marginal_i = p_conjunta.sum(axis=1)
p_marginal_j = p_conjunta.sum(axis=0)

# PPMI
eps = 1e-8
pmi = np.log2((p_conjunta + eps) / (np.outer(p_marginal_i, p_marginal_j) + eps))
ppmi = np.maximum(pmi, 0)

if "artificial" in word2idx:
    idx = word2idx["artificial"]
    print(f"Palabra: 'artificial'")
    print(f"Co-ocurrencias totales: {co_matrix[idx].sum()}")
    print(f"PPMI m치ximo: {ppmi[idx].max():.2f}")

# Seleccionar top N palabras para visualizaci칩n
top_n = 20
top_words = vocabulario[:top_n]
indices = [word2idx[word] for word in top_words]

# Submatriz reducida
submatrix = np.log1p(co_matrix[indices][:, indices])  # log(1 + x) para suavizar

# Configurar plot
plt.figure(figsize=(12, 10))
sns.heatmap(
    submatrix,
    annot=True,
    fmt=".1f",
    cmap="viridis",
    xticklabels=top_words,
    yticklabels=top_words,
)
plt.title("Log-Co-ocurrencias (Top 20 palabras)")
plt.xticks(rotation=45, ha="right")
plt.show()

# Obtener pares y sus conteos
pares = []
for i in range(vocab_size):
    for j in range(vocab_size):
        if co_matrix[i, j] > 0:
            pares.append((vocabulario[i], vocabulario[j], co_matrix[i, j]))

# Ordenar y seleccionar top 10
top_pares = sorted(pares, key=lambda x: x[2], reverse=True)[:10]
palabras1, palabras2, conteos = zip(*top_pares)

# Gr치fico de barras
plt.figure(figsize=(10, 6))
plt.barh(
    [f"{p1} - {p2}" for p1, p2 in zip(palabras1, palabras2)],
    conteos,
    color="skyblue",
)
plt.gca().invert_yaxis()
plt.xlabel("Frecuencia de co-ocurrencia")
plt.title("Top 10 pares de palabras co-ocurrentes")
plt.show()

# Reducci칩n a 2D
pca = PCA(n_components=2)
coords = pca.fit_transform(ppmi)

# Plot
plt.figure(figsize=(12, 8))
plt.scatter(coords[:, 0], coords[:, 1], alpha=0.5)
for i, word in enumerate(vocabulario[:50]):  # Primeras 50 palabras
    plt.annotate(word, (coords[i, 0], coords[i, 1]))
plt.title("Embeddings PPMI reducidos con PCA")
plt.show()

"""#Entrenamiento con word2vec"""

from gensim.models import word2vec
def load_model(model_path: str):
    try:
        print(model_path)
        return word2vec.Word2Vec.load(model_path)
    except:
        print(f"[WARN] Model not found in path {model_path}")
        return None

def report_stats(model) -> None:
    """Print report of a model"""
    print("Number of words in the corpus used for training the model: ", model.corpus_count)
    print("Number of words in the model: ", len(model.wv.index_to_key))
    print("Time [s], required for training the model: ", model.total_train_time)
    print("Count of trainings performed to generate this model: ", model.train_count)
    print("Length of the word2vec vectors: ", model.vector_size)
    print("Applied context length for generating the model: ", model.window)

from enum import Enum

class Algorithms(Enum):
    CBOW = 0
    SKIP_GRAM = 1

def train_model(sentences: list, model_name: str, vector_size: int, window=5, workers=2, algorithm = Algorithms.CBOW):
    model_name_params = f"{model_name}-vs{vector_size}-w{window}-{algorithm.name}.model"
    model_path = MODELS_DIR + model_name_params
    if load_model(model_path) is not None:
        print(f"Already exists the model {model_path}")
        return load_model(model_path)
    print(f"TRAINING: {model_path}")
    if algorithm in [Algorithms.CBOW, Algorithms.SKIP_GRAM]:
        print(f"Algorithm: {algorithm.name}")
        model = word2vec.Word2Vec(
            sentences,
            vector_size=vector_size,
            window=window,
            workers=workers,
            sg = algorithm.value,
            seed=42,
            )
    else:
        print("[ERROR] algorithm not implemented yet :p")
        return
    try:
        model.save(model_path)
    except:
        print(f"[ERROR] Saving model at {model_path}")
    return model

MODELS_DIR='/content/drive/MyDrive/Ciencia de Datos/8.Octavo Semestre/Temas selectos de PLN/Pr패acticas/Pra패ctica 3/models/word2vec/'
#print(MODELS_DIR)
#model_name = "eswiki-md-300-CBOW.model"
#model = load_model(MODELS_DIR + model_name)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# cbow_model = train_model(
#     docs_procesados,
#     "oscar",
#     vector_size=300,
#     window=5,
#     workers=2,
#     algorithm=Algorithms.CBOW
# )

report_stats(cbow_model)

cbow_model.wv["inteligencia"]

"""#Comparaci칩n entre Word2Vec y PPMI"""

# Palabras del modelo Word2Vec
words_word2vec = set(cbow_model.wv.key_to_index.keys())

# Palabras de la matriz PPMI
words_ppmi = set(vocabulario)

# Intersecci칩n de vocabularios
common_words = list(words_word2vec & words_ppmi)
print(f"Palabras comunes: {len(common_words)}")

# Seleccionar 100 palabras aleatorias
random.seed(42)
selected_words = random.sample(common_words, 100)

# Vectores Word2Vec
word2vec_vectors = np.array([cbow_model.wv[word] for word in selected_words])

# Vectores PPMI (usar los 칤ndices de la matriz)
ppmi_indices = [word2idx[word] for word in selected_words]
ppmi_vectors = ppmi[ppmi_indices]

# Se usa PCA para reducir dimensionalidad
pca = PCA(n_components=2)
word2vec_2d = pca.fit_transform(word2vec_vectors)

ppmi_2d = pca.fit_transform(ppmi_vectors)

plt.figure(figsize=(14, 7))

# Subplot para Word2Vec
plt.subplot(1, 2, 1)
plt.scatter(word2vec_2d[:, 0], word2vec_2d[:, 1], alpha=0.7, c='blue')
plt.title('Word2Vec (PCA)')
plt.grid(True, linestyle='--', alpha=0.5)

# Anotar algunas palabras clave
for i, word in enumerate(selected_words[:10]):  # Primeras 10 para claridad
    plt.annotate(word, (word2vec_2d[i, 0], word2vec_2d[i, 1]), fontsize=8)

# Subplot para PPMI
plt.subplot(1, 2, 2)
plt.scatter(ppmi_2d[:, 0], ppmi_2d[:, 1], alpha=0.7, c='red')
plt.title('PPMI (PCA)')
plt.grid(True, linestyle='--', alpha=0.5)

# Anotar mismas palabras para comparar
for i, word in enumerate(selected_words[:10]):
    plt.annotate(word, (ppmi_2d[i, 0], ppmi_2d[i, 1]), fontsize=8)

plt.tight_layout()
plt.show()

"""Lo que podemos observar es que en el caso de Word2Vec el rango de valores es mucho menor que en el caso de PPMI, pues como podemos observar, los valores en x van desde -4 hasta 1 para x y de -0.01 a 0.2 para y, por otro lado, para PPMI los valores van desde -10 hasta 25 en x y desde -10 hasta 25 para y, lo que nos habla de una mayor distancia entre los vectores. Al tratarse de las mismas palabras las relaciones sem치nticas deber칤an de ser similares y si ciertas palabras est치n relacionadas PPMI tambi칠n deber칤a de captar esto, sin embargo, podemos notar que hay una mayor dispersi칩n para el caso de PPMI, lo que nos habla de que es peor a la hora de captar relaciones sem치nticas. De este modo, la representaci칩n vectorial mediante Word2Vec es mejor para captar relaciones sem치nticas.


| Caracter칤stica          | Word2Vec              | PPMI                  |
|-------------------------|-----------------------|-----------------------|
| Relaciones sem치nticas    | Captura mejor         | Captura parcial       |
| Densidad informativa     | Alta (vectores densos)| Baja (matriz dispersa)|
| Interpretabilidad visual | Buena                 | Limitada              |
| Contextos locales        | Modela bien           | Modela expl칤citamente |
| Palabras raras           | Manejo regular        | Pobre manejo          |




"""