# Lab of Selected Themes on Computer Linguistics 2025-2

Repositorio para las pr치cticas de la materia Temas Selectos de Ling칲칤stica
Computacional en la Lic. de Ciencias de Datos, IIMAS, UNAM

## Instalaci칩n de dependencias

Usaremos `uv` como gestor de dependencias.

- [Instalaci칩n del programa](https://docs.astral.sh/uv/getting-started/installation/)

Una ves instalado basta con ejecutar el siguiente comando para levantar jupyter lab local con las dependencias:

```shell
$ uv run jupyter-lab
```

Si precisas instalar alguna dependencia puedes agregarla con:

```shell
$ uv add <dep>
```

**Toma en cuenta que si agregas dependencias estas se agregar치 al repositorio
principal (`pyproject.toml` y `uv.lock`) con tu PR asi que cuida lo que
precises instalar**

# Pr치cticas

### Lineamientos generales

- Es muy recomendable entregar las pr치cticas ya que representa un porcentaje
importante de su calificaci칩n (40%) 游뱁
- Se dar치 ~2 semanas para entregar ejercicios (dependiendo de la pr치ctica)
    - En caso de **entregas tard칤as** abr치 una penalizaci칩n `-1 punto` por cada d칤a
- Si utilizas LLMs, o herramientas generativas reportalos en tus pr치cticas 游븿
  - Les recomendamos ampliamente que lo intenten por su cuenta primero, es una
  oportunidad de enfrentarse a cosas nuevas y de pensar en soluciones por su
  cuenta :)

## 0. Creaci칩n de carpeta personal via *Pull Request (PR)*

Crear un PR con lo siguiente:

- Una carpeta con su username de GitHub dentro de `practicas/` y otra carpeta interna llamada `P0/`
    - `practicas/umoqnier/P0`
- Agrega un archivo llamado `README.md` a la carpeta `P0/` con informaci칩n b치sica sobre t칤. Ejemplo:
    - `practices/umoqnier/P0/README.md`
    - Usar lenguaje de marcado [Markdown](https://docs.github.com/es/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)

```markdown
$ cat README.md

# Diego Alberto Barriga Mart칤nez

- N칰mero de cuenta: `XXXXXXXX`
- User de Github: @umoqnier
- Me gusta que me llamen: Dieguito

## Pasatiempos

- Andar en bici

## Proyectos en los que he participado y que me enorgullesen 游둯

- [Esquite](https://github.com/ElotlMX/Esquite/)
```

## 1. Niveles Ling칲칤sticos

### FECHA DE ENTREGA: 16 de Febrero 2025 at 11:59pm

### Fon칠tica

1. Si tenemos un sistema de b칰squeda que recibe una palabra ortogr치fica y devuelve sus transcripciones fonol칩gicas, proponga una soluci칩n para los casos en que la palabra buscada no se encuentra en el lexic칩n/diccionario. *쮺칩mo devolver o aproximar su transcripci칩n fonol칩gica?*
  - Reutiliza el sistema de b칰squeda visto en clase y mejoralo con esta funcionalidad

### Morfolog칤a

2. Obtenga los datos de `test` y `dev` para todas las lenguas disponibles en el Shared Task SIGMORPHON 2022 y haga lo siguiente:
    - En un plot de 4 columnas y 2 rows muestre las siguientes distribuciones (un subplot por lengua):
        - Plot 1: distribuci칩n de longitud de palabras
        - Plot 2: distribuci칩n de la cuenta de morfemas
        - Plot 3: distribuci칩n de categorias (si existe para la lengua)
    - Realice una funci칩n que imprima por cada lengua lo siguiente:
        - Total de palabras
        - La longitud de palabra promedio
        - La cuenta de morfemas promedio
        - La categor칤a m치s com칰n
    - Con base en esta informaci칩n elabore una conclusi칩n ling칲칤stica sobre la morfolog칤a de las lenguas analizadas.
    
### EXTRA:

- Imprimir la [matr칤z de confusi칩n](https://en.wikipedia.org/wiki/Confusion_matrix) para el etiquetador CRFs visto en clase y elaborar una conclusi칩n sobre los resultados

## 2: Propiedades estad칤sticas de la lengua

### Fecha de entrega: 2 de Marzo de 2025 11:59pm

1. Verificar si la ley de Zipf se cumple en un lenguaje artificial creado por ustedes.
    - *Ejemplo:* Un "lenguaje artificial" podr칤a ser simplemente un texto donde las secuencias de caracteres fueron generadas aleatoriamente.
2. Explorar `datasets` del sitio [Hugging Face](https://huggingface.co/datasets) y elegir documentos de diferentes dominios en Espa침ol (al menos 3). Realizar reconocimiento de entidades nombradas (NER).
    - Pueden utilizar subconjuntos de los datasets encontrados
    - Mostrar resultados del reconocimiento
    - Una distribuci칩n de frecuencias de las etiquetas m치s comunes en cada dominio
    - Comentarios generales del desempe침o observado.

*Sugerencias: Spacy, CoreNLP (puede ser cualquier otra herramienta)*

## 3. Pr치ctica: Vectores a palabras

### Fecha de entrega: 19 de Marzo de 2025 @ 11:59pm

Obtenga la matriz de co-ocurrencia para un corpus en espa침ol y realice los siguientes calculos:
- Las probabilidades conjuntas
$$p(w_i,w_j) = \frac{c_{i,j}}{\sum_i \sum_j c_{i,j}}$$
- Las probabilidades marginales
$$p(w_i) = \sum_j p(w_i,w_j)$$
- Positive Point Wise Mutual Information (PPMI):
$$PPMI(w_i,w_j) = \max\{0, \log_2 \frac{p(w_i,w_j)}{p(w_i)p(w_j)}\}$$

**Comparaci칩n de representaciones**

Aplica reducci칩n de dimensionalidad (a 2D) de los vectores de la matr칤z con PPMI y de los vectores entrenados en espa침ol:

- Realiza un plot de 100 vectores aleatorios (que esten tanto en la matr칤z como en los vectores entrenados)
- Compara los resultados de los plots:
    - 쯈u칠 representaci칩n dir칤as que captura mejor relaciones sem치nticas?
    - Realiza un cuadro comparativo de ambos m칠todos con ventajas/desventajas

## Pr치ctica 4: Modelos del Lenguaje Neuronales

**Fecha de entrega: 6 de abril de 2025 11:59pm**

A partir del modelo entrenado:

- Sacar los embeddings de las palabras del vocabulario

- Visualizar en 2D los embeddings de algunas palabras (quiz치 las m치s frecuentes, excluyendo stopwords)

- Seleccione algunas palabras y verifique s칤 realmente codifican nociones sem치nticas, e,g, similitud sem치ntica con similitud coseno entre dos vectores, analog칤as por medios de operaciones de vectores

**NOTA**: Puedes entrenar el modelo replicando la ejecuci칩n del notebook o encontrar el modelo entrenado en la [carpeta de drive](https://drive.google.com/drive/folders/1Mq-UA0ct5iTp-7h8-SxJxwyjdMHXmwO4?usp=drive_link)

### Extra (0.5 pts):

- Correr el modelo de Bengio pero aplicando una t칠cnica de subword tokenization al corpus y hacer generaci칩n del lenguaje

- La generaci칩n del lenguaje debe ser secuencias de palabras (no subwords)

## Pr치ctica 5: Tech evolution. Caso *POS Tagging*

**Fecha de entrega: 13 de Abril 2025 11:59pm**

- Obten los embeddings de 100 palabras al azar del modelo RNN visto en clase
  - Pueden ser los embeddings est치ticos o los din치micos del modelo
- Aplica un algoritmo de clusterizaci칩n a las palabras y plotearlas en 2D
  - Aplica algun color para los diferentes clusters
- Agrega al plot los embeddings de las etiquetas POS
  - Utiliza un marcador que las distinga claramente de las palabras
- Realiza una conclusi칩n sobre los resultados observados

### Extra: 0.5pt

- Implementa una red *Long short-term memory units (LSTM)* para la tarea de etiquetado POS
- Reporta el accuracy y comparalo con los resultados de la RNN simple
- Realiza un comentario sobre como impacta la arquitectura LSTM sobre el resultado obtenido
